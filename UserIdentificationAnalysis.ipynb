{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnEf299z3qfE"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WP22ySGZLWrN",
    "outputId": "735086c3-4933-42ae-ecf6-d21171354997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: netcal in /usr/local/lib/python3.10/dist-packages (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from netcal) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.10/dist-packages (from netcal) (1.11.4)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from netcal) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from netcal) (1.2.2)\n",
      "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from netcal) (2.3.1+cu121)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from netcal) (0.18.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.10/dist-packages (from netcal) (4.66.4)\n",
      "Requirement already satisfied: pyro-ppl>=1.8 in /usr/local/lib/python3.10/dist-packages (from netcal) (1.9.1)\n",
      "Requirement already satisfied: tikzplotlib==0.9.8 in /usr/local/lib/python3.10/dist-packages (from netcal) (0.9.8)\n",
      "Requirement already satisfied: tensorboard>=2.2 in /usr/local/lib/python3.10/dist-packages (from netcal) (2.15.2)\n",
      "Requirement already satisfied: gpytorch>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from netcal) (1.12)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tikzplotlib==0.9.8->netcal) (9.4.0)\n",
      "Requirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch>=1.5.1->netcal) (1.3.0)\n",
      "Requirement already satisfied: linear-operator>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from gpytorch>=1.5.1->netcal) (0.5.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->netcal) (2.8.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8->netcal) (3.3.0)\n",
      "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from pyro-ppl>=1.8->netcal) (0.1.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->netcal) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->netcal) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (1.4.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (1.64.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (2.27.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (2.31.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (67.7.2)\n",
      "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.2->netcal) (3.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (1.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->netcal) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.9->netcal) (12.5.82)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2->netcal) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2->netcal) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2->netcal) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2->netcal) (1.3.1)\n",
      "Requirement already satisfied: jaxtyping>=0.2.9 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.2->gpytorch>=1.5.1->netcal) (0.2.33)\n",
      "Requirement already satisfied: typeguard~=2.13.3 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.2->gpytorch>=1.5.1->netcal) (2.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->netcal) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->netcal) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->netcal) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2->netcal) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2->netcal) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2->netcal) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2->netcal) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "#!python3 -m pip install netcal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8vbzmkeI4_i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from scipy.stats.mstats import gmean\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.signal import savgol_filter\n",
    "import glob\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# from dtaidistance import dtw\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "r95SYeFplinj",
    "outputId": "96c88e94-38c5-4097-e2ad-8ef1ab94c32d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "tf.keras.backend.floatx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeZT1INMVkCI"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pGWzwdfOxs3"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3yxSrGp3u-E"
   },
   "source": [
    "# Data Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwtoWYsW3xOt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self, decision_size, decision_overlap, segments_size=90, segments_overlap=45, sampling=2):\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.sampling = sampling\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "    def transfer(self, dataset, features, method):\n",
    "        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n",
    "        segments, labels = self.__segment_signal(dataset, features)\n",
    "        print(\"making \" + str(len(segments)) + \" segments\")\n",
    "        if method == \"table\":\n",
    "            segments_dataset = self.__transfer_table(segments, features)\n",
    "        elif method == \"1d\":\n",
    "            segments_dataset = self.__transfer_1d(segments, features)\n",
    "        elif method == \"2d\":\n",
    "            segments_dataset = self.__transfer_2d(segments, features)\n",
    "        elif method == \"3d_1ch\":\n",
    "            segments_dataset = self.__transfer_2d_1ch(segments, features)\n",
    "        elif method == \"3d\":\n",
    "            segments_dataset = self.__transfer_3d(segments, features)\n",
    "        elif method == \"4d\":\n",
    "            segments_dataset = self.__transfer_4d(segments, features)\n",
    "        elif method == \"rnn_2d\":\n",
    "            segments_dataset, labels =  self.__transfer_rnn_2d(segments, labels, features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            segments_dataset, labels =  self.__transfer_rnn_3d_1ch(segments, labels, features)\n",
    "        return segments_dataset, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def data_shape(method, n_features, segments_size, segments_overlap=None, decision_size=None):\n",
    "        if method == \"table\":\n",
    "            return (None, n_features * segments_size)\n",
    "        elif method == \"1d\":\n",
    "            return (None, 1, n_features * segments_size, 1)\n",
    "        elif method == \"2d\":\n",
    "            return (None, n_features, segments_size)\n",
    "        elif method == \"3d_1ch\":\n",
    "            return (None, n_features, segments_size, 1)\n",
    "        elif method == \"3d\":\n",
    "            return (None, 1, segments_size, n_features)\n",
    "        elif method == \"4d\":\n",
    "            return (n_features, None, 1, segments_size, 1)\n",
    "        elif method == \"rnn_2d\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, segments_size*n_features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, 1, segments_size*n_features)\n",
    "        return ()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_segments_a_decision_window(segment_size, segment_overlap_size, decision_size):\n",
    "        return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)\n",
    "\n",
    "    def __transfer_rnn_2d(self, segments, labels, features):\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "            subset = segments[np.where(labels == _id)]\n",
    "            n = subset.shape[0]\n",
    "            o = int(np.floor((n - r) / (s - r)))\n",
    "            for i in range(o):\n",
    "                row = []\n",
    "                for j in range(s):\n",
    "                    A = subset[i * (s-r) + j]\n",
    "                    A = A.reshape(A.shape[0]*A.shape[1])\n",
    "                    row.append(A)\n",
    "                y_output.append(_id)\n",
    "                x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_rnn_3d_1ch(self, segments, labels, features):\n",
    "        # (samples, time, channels=1, rows)\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "            subset = segments[np.where(labels == _id)]\n",
    "            n = subset.shape[0]\n",
    "            o = int(np.floor((n - r) / (s - r)))\n",
    "            for i in range(o):\n",
    "                row = []\n",
    "                for j in range(s):\n",
    "                    A = subset[i * (s-r) + j]\n",
    "                    A = A.reshape(A.shape[0]*A.shape[1])\n",
    "                    row.append([A])\n",
    "                y_output.append(_id)\n",
    "                x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_table(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_1d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_2d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_3d_1ch(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_3d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for i in range(len(segment[0])):\n",
    "                cell = []\n",
    "                for feature_i in range(len(features)):\n",
    "                    cell.append(segment[feature_i][i])\n",
    "                row.append(cell)\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_4d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for feature_i in range(len(features)):\n",
    "            row = []\n",
    "            for segment in segments:\n",
    "                cell = []\n",
    "                for element in segment[feature_i]:\n",
    "                    cell.append([element])\n",
    "                row.append([cell])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __windows(self, data):\n",
    "        start = 0\n",
    "        while start < data.count():\n",
    "            yield int(start), int(start + self.segments_size)\n",
    "            start += (self.segments_size - self.segments_overlap)\n",
    "\n",
    "    def __segment_signal(self, dataset, features):\n",
    "        segments = []\n",
    "        labels = []\n",
    "        for class_i in np.unique(dataset[\"id\"]):\n",
    "            subset = dataset[dataset[\"id\"] == class_i]\n",
    "            for (start, end) in self.__windows(subset[\"id\"]):\n",
    "                feature_slices = []\n",
    "                for feature in features:\n",
    "                    feature_slices.append(subset[feature][start:end].tolist())\n",
    "                if len(feature_slices[0]) == self.segments_size:\n",
    "                    segments.append(feature_slices)\n",
    "                    labels.append(class_i)\n",
    "        return np.array(segments), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsqjDnvY30UY"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RisgNiya33cE"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 label_noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        \"\"\"\n",
    "        :param db_path:\n",
    "        :param sample_rate:\n",
    "        :param features:\n",
    "        :param window_time: in seconds\n",
    "        :param window_overlap_percentage: example: 0.75 for 75%\n",
    "        :param add_noise: True or False\n",
    "        :param noise_rate:\n",
    "        :param train_blocks:\n",
    "        :param valid_blocks:\n",
    "        :param test_blocks:\n",
    "        :param data_length_time: the amount of data from each class in seconds. -1 means whole existing data.\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.features = features\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.label_noise_rate = label_noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "\n",
    "        # Initialization\n",
    "        self.train_dataset = pd.DataFrame()\n",
    "        self.valid_dataset = pd.DataFrame()\n",
    "        self.test_dataset = pd.DataFrame()\n",
    "        self.n_train_dataset = pd.DataFrame()\n",
    "        self.n_valid_dataset = pd.DataFrame()\n",
    "        self.n_test_dataset = pd.DataFrame()\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "    def load_data(self, n_classes, method, subset=None, read_cache=True, random_selection=True):\n",
    "        segments_path = self.db_path + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method \" + str(method) + os.sep + \\\n",
    "                        \"noise \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test \" + str(self.test_blocks) + os.sep\n",
    "        print(segments_path)\n",
    "        if read_cache \\\n",
    "                and os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            self.__preprocess(n_classes, method, subset, random_selection)\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "        def to_dic(data):\n",
    "            dic = {}\n",
    "            for i, x in enumerate(data):\n",
    "                dic[str(i)] = x\n",
    "            return dic\n",
    "\n",
    "        if len(self.X_train.shape) == 5:\n",
    "            self.X_train = to_dic(self.X_train)\n",
    "            self.X_valid = to_dic(self.X_valid)\n",
    "            self.X_test = to_dic(self.X_test)\n",
    "\n",
    "    def __preprocess(self, n_classes, method, subset=None, random_selection=True):\n",
    "        if random_selection:\n",
    "            csv_paths = np.random.choice(glob.glob(self.db_path + \"*.csv\"), n_classes, replace=False)\n",
    "        else:\n",
    "            csv_paths = glob.glob(self.db_path + \"*.csv\")[:n_classes]\n",
    "\n",
    "        self.class_names = {}\n",
    "        for i, csv_path in enumerate(csv_paths):\n",
    "            label = os.path.basename(csv_path).split('.')[0]\n",
    "            self.class_names[label] = i\n",
    "            train, valid, test = self.__read_data(csv_path, self.features, label, subset)\n",
    "            train['id'] = i\n",
    "            valid['id'] = i\n",
    "            test['id'] = i\n",
    "\n",
    "            def flip_labels(labels, num_classes, flip_percentage):\n",
    "                num_samples = labels.shape[0]\n",
    "                num_flips = int(num_samples * flip_percentage)\n",
    "\n",
    "                # Randomly select indices to flip\n",
    "                flip_indices = np.random.choice(num_samples, num_flips, replace=False)\n",
    "                # print(flip_indices)\n",
    "\n",
    "                # Generate new random labels for flipped indices\n",
    "                new_labels = np.random.randint(0, num_classes, num_flips)\n",
    "\n",
    "                # Update the labels with flipped values\n",
    "                labels[flip_indices] = new_labels\n",
    "\n",
    "                return labels\n",
    "\n",
    "            if self.label_noise_rate > 0:\n",
    "                train['id'] = flip_labels(train['id'].to_numpy(), len(csv_paths), self.label_noise_rate)\n",
    "\n",
    "\n",
    "            self.train_dataset = pd.concat([self.train_dataset, train])\n",
    "            self.valid_dataset = pd.concat([self.valid_dataset, valid])\n",
    "            self.test_dataset = pd.concat([self.test_dataset, test])\n",
    "\n",
    "        self.__standardization()\n",
    "        self.__segmentation(method=method)\n",
    "\n",
    "    def __read_data(self, path, features, label, subset=None):\n",
    "        data = pd.read_csv(path, low_memory=False)\n",
    "        if subset != None:\n",
    "            data = data.loc[ data['activity'].isin(subset) ]\n",
    "        data = data[features]\n",
    "        data = data.fillna(data.mean())\n",
    "\n",
    "        # plt.plot(data)\n",
    "        # data = data.rolling(window=5).mean()\n",
    "        # plt.plot(data)\n",
    "        # plt.show()\n",
    "\n",
    "        length = self.data_length_size if self.data_length_size != -1 else data.shape[0]\n",
    "        print('class: %5s, data size: %s, selected data size: %s' % (\n",
    "            label, str(timedelta(seconds=int(data.shape[0] / self.sample_rate))),\n",
    "            str(timedelta(seconds=int(length / self.sample_rate)))))\n",
    "        return self.__split_to_train_valid_test(data)\n",
    "\n",
    "    def __split_to_train_valid_test(self, data):\n",
    "        n_blocks = max(self.train_blocks + self.valid_blocks + self.test_blocks) + 1\n",
    "        block_length = int(len(data[:self.data_length_size]) / n_blocks)\n",
    "\n",
    "        train_data = pd.DataFrame()\n",
    "        for i in range(len(self.train_blocks)):\n",
    "            start = self.train_blocks[i] * block_length\n",
    "            end = self.train_blocks[i] * block_length + block_length - 1\n",
    "            if train_data.empty:\n",
    "                train_data = data[start:end]\n",
    "            else:\n",
    "                train_data = pd.concat([data[start:end], train_data])\n",
    "\n",
    "        valid_data = pd.DataFrame()\n",
    "        for i in range(len(self.valid_blocks)):\n",
    "            start = self.valid_blocks[i] * block_length\n",
    "            end = self.valid_blocks[i] * block_length + block_length - 1\n",
    "            if valid_data.empty:\n",
    "                valid_data = data[start:end]\n",
    "            else:\n",
    "                valid_data = pd.concat([data[start:end], valid_data])\n",
    "\n",
    "        test_data = pd.DataFrame()\n",
    "        for i in range(len(self.test_blocks)):\n",
    "            start = self.test_blocks[i] * block_length\n",
    "            end = self.test_blocks[i] * block_length + block_length - 1\n",
    "            if test_data.empty:\n",
    "                test_data = data[start:end]\n",
    "            else:\n",
    "                test_data = pd.concat([data[start:end], test_data])\n",
    "\n",
    "        if self.add_noise:\n",
    "            test_data = self.__add_noise_to_data(test_data)\n",
    "\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def __add_noise_to_data(self, x):\n",
    "        x_power = x ** 2\n",
    "        sig_avg_watts = np.mean(x_power)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)\n",
    "        noise_avg_db = sig_avg_db - self.target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), size=x.shape)\n",
    "        return x + noise_volts\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.train_dataset.iloc[:, :-1])\n",
    "        n_train_dataset = scaler.transform(self.train_dataset.iloc[:, :-1])\n",
    "        n_valid_dataset = scaler.transform(self.valid_dataset.iloc[:, :-1])\n",
    "        n_test_dataset = scaler.transform(self.test_dataset.iloc[:, :-1])\n",
    "\n",
    "        self.n_train_dataset = pd.DataFrame(n_train_dataset, columns=self.features)\n",
    "        self.n_valid_dataset = pd.DataFrame(n_valid_dataset, columns=self.features)\n",
    "        self.n_test_dataset = pd.DataFrame(n_test_dataset, columns=self.features)\n",
    "        self.n_train_dataset['id'] = self.train_dataset.iloc[:, -1].tolist()\n",
    "        self.n_valid_dataset['id'] = self.valid_dataset.iloc[:, -1].tolist()\n",
    "        self.n_test_dataset['id'] = self.test_dataset.iloc[:, -1].tolist()\n",
    "\n",
    "    def __segmentation(self, method):\n",
    "        transformer = Transformer(segments_size=self.window_size,\n",
    "                                  segments_overlap=self.window_overlap_size,\n",
    "                                  decision_size=self.decision_size,\n",
    "                                  decision_overlap=self.decision_overlap_percentage)\n",
    "        self.X_train, self.y_train = transformer.transfer(self.n_train_dataset, self.features, method=method)\n",
    "        self.X_valid, self.y_valid = transformer.transfer(self.n_valid_dataset, self.features, method=method)\n",
    "        self.X_test, self.y_test = transformer.transfer(self.n_test_dataset, self.features, method=method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJE0iXH6a0nk"
   },
   "source": [
    "# Dataset (DB2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzpHdqt3a1-O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from random import sample\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "\n",
    "class Dataset2(Dataset):\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 label_noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        self.w = window_time\n",
    "        self.r = self.w - int(self.w * window_overlap_percentage)\n",
    "        self.db_path = db_path\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.label_noise_rate = label_noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "        self.features = features\n",
    "\n",
    "    def load_data(self, n_classes, method):\n",
    "        segments_path = self.db_path + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method \" + str(method) + os.sep + \\\n",
    "                        \"noise \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test \" + str(self.test_blocks) + os.sep\n",
    "        if os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            y_signals = self.load_y(self.db_path+\"train/y_train.txt\")\n",
    "\n",
    "            instances = len(y_signals)\n",
    "            total = np.array([i for i in range(instances)])\n",
    "            valids = sample([i for i in range(instances)], int(instances * 0.3))\n",
    "            trains = np.delete(total, valids).tolist()\n",
    "\n",
    "            self.X_train, self.y_train = self.load_X(self.db_path + \"train/Inertial Signals/\", y_signals, samples=trains, w=self.w, r=self.r)\n",
    "            self.X_valid, self.y_valid = self.load_X(self.db_path + \"train/Inertial Signals/\", y_signals, samples=valids, w=self.w, r=self.r)\n",
    "\n",
    "            y_signals = self.load_y(self.db_path + \"test/y_test.txt\")\n",
    "            self.X_test, self.y_test = self.load_X(self.db_path + \"test/Inertial Signals/\", y_signals, w=self.w, r=self.r)\n",
    "\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "\n",
    "    def windows(self, data, w, r):\n",
    "        start = 0\n",
    "        while start < len(data):\n",
    "            yield int(start), int(start + w)\n",
    "            start += (w - r)\n",
    "\n",
    "    def segment_signal(self, X, y, w, r):\n",
    "        X_new = []\n",
    "        y_new = []\n",
    "        for (start, end) in self.windows(X, w, r):\n",
    "            segment = X[start:end]\n",
    "            if len(segment) == w:\n",
    "                X_new.append(segment)\n",
    "                y_new.append(y-1)\n",
    "        return np.array(X_new), np.array(y_new)\n",
    "\n",
    "    def load_y(self, y_path):\n",
    "        file = open(y_path, 'r')\n",
    "        # Read dataset from disk, dealing with text file's syntax\n",
    "        y_ = np.array(\n",
    "            [elem for elem in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        file.close()\n",
    "        return np.array(y_, dtype=np.int32)\n",
    "\n",
    "    def load_X(self, path, y, samples=None, w=32, r=8):\n",
    "        X_signals = []\n",
    "        y_signals = []\n",
    "        files = os.listdir(path)\n",
    "        files.sort(key=str.lower)\n",
    "        files = glob.glob(path + \"*.txt\")\n",
    "\n",
    "        for fileName in files:\n",
    "            print(fileName)\n",
    "            file = open(fileName, 'r')\n",
    "            X_container = []\n",
    "            y_container = []\n",
    "\n",
    "            for i, row in enumerate(file):\n",
    "                if samples != None and i not in samples:\n",
    "                    continue\n",
    "                row = row.strip().replace('  ', ' ')\n",
    "                X_i, y_i = self.segment_signal(row.strip().split(' '), y=y[i][0], w=w, r=r)\n",
    "                X_container.append(np.array(X_i, dtype=np.float32))\n",
    "                y_container.append(np.array(y_i, dtype=np.float32))\n",
    "\n",
    "            file.close()\n",
    "\n",
    "            X_container = np.array(X_container)\n",
    "            X_container = X_container.reshape(-1, w)\n",
    "            X_signals.append(X_container)\n",
    "\n",
    "            y_container = np.array(y_container)\n",
    "            y_container = y_container.reshape(-1, 1)\n",
    "            y_signals.append(y_container)\n",
    "\n",
    "        X_signals = np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "        X_signals = X_signals.reshape(-1 , 1, w, len(self.features))\n",
    "        return np.array(X_signals), y_signals[0].reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NRgkdXOYoky"
   },
   "source": [
    "# Dataset (HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wi0A1QPTYrsU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "from random import sample\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Dataset3(Dataset):\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 label_noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        self.w = window_time\n",
    "        self.r = self.w - int(self.w * window_overlap_percentage)\n",
    "        self.db_path = db_path\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.label_noise_rate = label_noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "        self.features = features\n",
    "\n",
    "    def load_data(self, n_classes, method):\n",
    "        segments_path = self.db_path + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method \" + str(method) + os.sep + \\\n",
    "                        \"noise \" + str(self.noise_rate) + os.sep + \\\n",
    "                        \"n_classes\" + str(n_classes) + os.sep + \\\n",
    "                        \"wl \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test \" + str(self.test_blocks) + os.sep\n",
    "        if os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            y_signals = self.load_y(self.db_path+\"train/y_train.txt\")\n",
    "            self.X_train, self.y_train = self.load_X(self.db_path + \"train/Inertial Signals/\", y_signals, w=self.w, r=self.r)\n",
    "\n",
    "            y_signals = self.load_y(self.db_path + \"test/y_test.txt\")\n",
    "            self.X_test, self.y_test = self.load_X(self.db_path + \"test/Inertial Signals/\", y_signals, w=self.w, r=self.r)\n",
    "\n",
    "            X = np.concatenate((self.X_train, self.X_test), axis=0)\n",
    "            y = np.concatenate((self.y_train, self.y_test), axis=0)\n",
    "\n",
    "            X_temp, self.X_valid, y_temp, self.y_valid = train_test_split(X, y, test_size=0.2, stratify=y, random_state=0)\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_temp, y_temp, test_size=0.625, stratify=y_temp, random_state=0)\n",
    "\n",
    "            # self.__standardization()\n",
    "\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.X_train)\n",
    "        self.X_train = scaler.transform(self.X_train)\n",
    "        self.X_valid = scaler.transform(self.X_valid)\n",
    "        self.X_test = scaler.transform(self.X_test)\n",
    "\n",
    "    def windows(self, data, w, r):\n",
    "        start = 0\n",
    "        while start < len(data):\n",
    "            yield int(start), int(start + w)\n",
    "            start += (w - r)\n",
    "\n",
    "    def segment_signal(self, X, y, w, r):\n",
    "        X_new = []\n",
    "        y_new = []\n",
    "        for (start, end) in self.windows(X, w, r):\n",
    "            segment = X[start:end]\n",
    "            if len(segment) == w:\n",
    "                X_new.append(segment)\n",
    "                y_new.append(y)\n",
    "        return np.array(X_new), np.array(y_new)\n",
    "\n",
    "    def load_y(self, y_path):\n",
    "        file = open(y_path, 'r')\n",
    "        # Read dataset from disk, dealing with text file's syntax\n",
    "        y_ = np.array(\n",
    "            [elem for elem in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]],\n",
    "            dtype=np.int32\n",
    "        )\n",
    "        file.close()\n",
    "        return np.array(y_, dtype=np.int32)\n",
    "\n",
    "    def load_X(self, path, y, samples=None, w=32, r=8):\n",
    "        X_signals = []\n",
    "        y_signals = []\n",
    "        files = os.listdir(path)\n",
    "        files.sort(key=str.lower)\n",
    "        files = glob.glob(path + \"*.txt\")\n",
    "\n",
    "        for fileName in files:\n",
    "            print(fileName)\n",
    "            file = open(fileName, 'r')\n",
    "            X_container = []\n",
    "            y_container = []\n",
    "\n",
    "            for i, row in enumerate(file):\n",
    "                if samples != None and i not in samples:\n",
    "                    continue\n",
    "                row = row.strip().replace('  ', ' ')\n",
    "                X_i, y_i = self.segment_signal(row.strip().split(' '), y=y[i][0], w=w, r=r)\n",
    "                X_container.append(np.array(X_i, dtype=np.float32))\n",
    "                y_container.append(np.array(y_i, dtype=np.float32))\n",
    "\n",
    "            file.close()\n",
    "\n",
    "            X_container = np.array(X_container)\n",
    "            X_container = X_container.reshape(-1, w)\n",
    "            X_signals.append(X_container)\n",
    "\n",
    "            y_container = np.array(y_container)\n",
    "            y_container = y_container.reshape(-1, 1)\n",
    "            y_signals.append(y_container)\n",
    "\n",
    "        X_signals = np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "        X_signals = X_signals.reshape(-1 , 1, w, len(self.features))\n",
    "        return np.array(X_signals), y_signals[0].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTAGp_3s364X"
   },
   "source": [
    "# Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNfOUKPl4Crf"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def pesl(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = (c - 1) / (c ** 2)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    return tf.math.reduce_mean(tf.math.reduce_mean(tf.math.square(tf.math.subtract(y, q)), axis=1) + payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnkKOUHKk6GY"
   },
   "outputs": [],
   "source": [
    "def esl(y,q):\n",
    "    return tf.math.reduce_mean(tf.keras.losses.mean_squared_error(y,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8OsafN9S5aP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def pll(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = math.log(1/c)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    log_loss = tf.keras.losses.categorical_crossentropy(y,q)\n",
    "    p_log_loss = tf.cast(log_loss, tf.float32) - payoff\n",
    "    return tf.math.reduce_mean(p_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmmPDBs5k4ca"
   },
   "outputs": [],
   "source": [
    "def ll(y,q):\n",
    "    return tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(y,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KP0pEQM4KNT"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2prG7BJT4Kt5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    # Define the true positives, false positives and false negatives\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    # Calculate the precision and recall\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEfQ8vJyOYDS"
   },
   "source": [
    "# Calibration Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r7V3R5CP_Ud"
   },
   "outputs": [],
   "source": [
    "# Import Uncertainty Toolbox\n",
    "# import uncertainty_toolbox as uct\n",
    "from netcal.binning import HistogramBinning, IsotonicRegression, ENIR, BBQ\n",
    "from netcal.scaling import LogisticCalibration, BetaCalibration, TemperatureScaling\n",
    "from netcal.presentation import ReliabilityDiagram\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from netcal.metrics import ECE, ACE, MCE, MMCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnB_Uq_cOauh"
   },
   "outputs": [],
   "source": [
    "class Calibrator():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.recal_model = None\n",
    "        if self.model == 'TemperatureScaling':\n",
    "            self.recal_model = TemperatureScaling()\n",
    "        elif self.model == 'BetaCalibration':\n",
    "            self.recal_model = BetaCalibration()\n",
    "        elif self.model == 'LogisticCalibration':\n",
    "            self.recal_model = LogisticCalibration()\n",
    "        elif self.model == 'HistogramBinning':\n",
    "            self.recal_model = HistogramBinning(20)\n",
    "        elif self.model == 'IsotonicRegression':\n",
    "            self.recal_model = IsotonicRegression()\n",
    "        elif self.model == 'ENIR':\n",
    "            self.recal_model = ENIR()\n",
    "        elif self.model == 'BBQ':\n",
    "            self.recal_model = BBQ()\n",
    "\n",
    "    def fit(self, confidences, y_real):\n",
    "        if self.model == 'TemperatureScaling' or self.model == 'BetaCalibration' or self.model == 'LogisticCalibration':\n",
    "            self.recal_model.fit(confidences, y_real)\n",
    "        elif self.model == 'HistogramBinning' or self.model == 'IsotonicRegression' or self.model == 'ENIR' or self.model == 'BBQ':\n",
    "            self.recal_model.fit(confidences, y_real)\n",
    "\n",
    "\n",
    "    def transform(self, confidences):\n",
    "        if self.model == 'TemperatureScaling' or self.model == 'BetaCalibration' or self.model == 'LogisticCalibration':\n",
    "            return self.recal_model.transform(confidences)\n",
    "        elif self.model == 'HistogramBinning' or self.model == 'IsotonicRegression' or self.model == 'ENIR' or self.model == 'BBQ':\n",
    "            return self.recal_model.transform(confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtXwjcAvtR4K"
   },
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    x = x - x.max(axis=axis, keepdims=True)\n",
    "    y = np.exp(x)\n",
    "    return y / y.sum(axis=axis, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4u0TcOVSzGi3",
    "outputId": "b10d4876-239c-4a56-e03e-ea9d765acea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25462853, 0.28140804, 0.46396343],\n",
       "       [0.00245611, 0.00667641, 0.99086747],\n",
       "       [0.21194156, 0.21194156, 0.57611688]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax( np.array([[0.1,0.2,0.7],[1,2,7],[0,0,1]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jM3l01B4R-Y"
   },
   "source": [
    "# MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byq18z6w4K5r"
   },
   "outputs": [],
   "source": [
    "class Classifier():\n",
    "    def __init__(self, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        return self.loss_function\n",
    "\n",
    "    def get_loss_function_name(self):\n",
    "        return self.loss_function if type(self.loss_function) == str else self.loss_function.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95NTs4jw4aN8"
   },
   "outputs": [],
   "source": [
    "# Convolutinal Neural Network\n",
    "class CNN_L(Classifier):\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap,\n",
    "                 decision_size, decision_overlap,\n",
    "                 loss_function, loss_metric, last_activation,\n",
    "                 lr=0.0001, beta_1=0.5, training=False):\n",
    "        super().__init__(loss_metric)\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal(seed=np.random.seed(1337))\n",
    "        self.last_activation = last_activation\n",
    "        self.training = training\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Nadam(lr, beta_1)\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy',\n",
    "                                    #tf.keras.metrics.Precision(),\n",
    "                                    #tf.keras.metrics.Recall(),\n",
    "                                    # f1_metric,\n",
    "                                    # loss_metric\n",
    "                                    ])\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"3d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def get_dropout(self, input_tensor, p=0.2, mc=False):\n",
    "        if mc:\n",
    "            return tf.keras.layers.Dropout(p)(input_tensor, training=True)\n",
    "        else:\n",
    "            return tf.keras.layers.Dropout(p)(input_tensor)\n",
    "\n",
    "    def build_model_l(self):\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(input_)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = self.get_dropout(x, 0.2, mc=self.training)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = self.get_dropout(x, 0.2, mc=self.training)\n",
    "        x = tf.keras.layers.Conv2D(16, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = self.get_dropout(x, 0.2, mc=self.training)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(x)\n",
    "        dense = tf.keras.layers.Dense(1024, activation='relu', kernel_initializer=self.initializer, bias_initializer=self.initializer)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = self.get_dropout(dense, 0.2, mc=self.training)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation=self.last_activation)(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, callback, monitor_mode, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "\n",
    "        history = self.model.fit(X_train, y_train_onehot,\n",
    "                                 validation_data=(X_valid, y_valid_onehot),\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=1,\n",
    "                                 shuffle=True,\n",
    "                                 callbacks=callback)\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJsGcm6jBgAL"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKIV9uFSCnKA"
   },
   "outputs": [],
   "source": [
    "class ModelAnalyser:\n",
    "\n",
    "    def __init__(self, segment_size, segment_overlap, decision_size, decision_overlap, X, y):\n",
    "        \"\"\"\n",
    "        :param segment_size: for calculating number of segments in a decision window.\n",
    "        :param segment_overlap: for calculating number of segments in a decision window.\n",
    "        :param decision_size:\n",
    "        :param decision_overlap:\n",
    "        :param X:\n",
    "        :param y:\n",
    "        \"\"\"\n",
    "        self.segments_a_decision_window = get_segments_a_decision_window(segment_size, segment_overlap, decision_size)\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.X = X\n",
    "        self.y_real = y\n",
    "        self.y_real_onehot = np.asarray(pd.get_dummies(y), dtype=np.int8)\n",
    "\n",
    "    def measurement(self, y_prediction, value_coef, monitor):\n",
    "        \"\"\"\n",
    "        :param y_real: expected labels\n",
    "        :param y_prediction: the outcomes of core\n",
    "        :param monitor: '#METHOD_#MEASURE', #METHOD={'mv','ms'}, #MEASURE={'loss','accuracy','precision','recall','f1'}\n",
    "        \"\"\"\n",
    "\n",
    "        monitor_method = monitor.split('_')[0]\n",
    "        monitor_measure = monitor.split('_')[1]\n",
    "\n",
    "        if monitor_method == 'ms':\n",
    "            y_pred_labels, y_dw_real = AveragingProbabilities(y_truth=self.y_real,\n",
    "                                                              y_prediction=y_prediction,\n",
    "                                                              s=self.segments_a_decision_window,\n",
    "                                                              r=self.decision_overlap,\n",
    "                                                              weights=value_coef)\n",
    "\n",
    "            y_pred_one_hot = np.zeros_like(y_pred_labels)\n",
    "            y_pred_one_hot[np.arange(len(y_pred_one_hot)), y_pred_labels.argmax(1)] = 1\n",
    "            if monitor_measure == 'mse':\n",
    "                loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                return loss_fn(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pesl':\n",
    "                return pesl(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pll':\n",
    "                return pll(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'accuracy':\n",
    "                return accuracy_score(y_dw_real, y_pred_one_hot)\n",
    "            elif monitor_measure == 'precision':\n",
    "                return precision_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'recall':\n",
    "                return recall_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'f1':\n",
    "                return f1_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "\n",
    "\n",
    "        if monitor_method == 'mv':\n",
    "            y_pred_labels, y_dw_real = MajorityVote(y_truth=self.y_real,\n",
    "                                                    y_prediction=y_prediction,\n",
    "                                                    s=self.segments_a_decision_window,\n",
    "                                                    r=self.decision_overlap,\n",
    "                                                    weights=value_coef)\n",
    "\n",
    "            y_pred_one_hot = np.zeros_like(y_pred_labels)\n",
    "            y_pred_one_hot[np.arange(len(y_pred_one_hot)), y_pred_labels.argmax(1)] = 1\n",
    "            if monitor_measure == 'mse':\n",
    "                loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "                return loss_fn(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pesl':\n",
    "                return pesl(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'pll':\n",
    "                return pll(y_dw_real, y_pred_labels).numpy()\n",
    "            elif monitor_measure == 'accuracy':\n",
    "                return accuracy_score(y_dw_real, y_pred_one_hot)\n",
    "            elif monitor_measure == 'precision':\n",
    "                return precision_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'recall':\n",
    "                return recall_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "            elif monitor_measure == 'f1':\n",
    "                return f1_score(y_dw_real, y_pred_one_hot, average='macro')\n",
    "\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urkt3wz-Pe_q"
   },
   "source": [
    "# UN Logging Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-KYM_a1-LhW"
   },
   "outputs": [],
   "source": [
    "class UNLoggingCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, mc_model, metric: ModelAnalyser, mode, monitor, last_activation, calibrator_title, mc_sn, checkpoint_dir):\n",
    "        super(tf.keras.callbacks.Callback, self).__init__()\n",
    "        self.mc_model = mc_model\n",
    "        self.metric = metric\n",
    "        self.monitor = monitor\n",
    "        self.monitor_method = monitor.split('_')[0]\n",
    "        self.monitor_measure = monitor.split('_')[1]\n",
    "        self.calibrator_title = calibrator_title\n",
    "        self.last_activation = last_activation\n",
    "        self.mc_sn = mc_sn\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        C = len(np.unique(self.metric.y_real))\n",
    "\n",
    "        latest = tf.train.latest_checkpoint(self.checkpoint_dir)\n",
    "        if latest != None:\n",
    "          print(\"loading weights to mc model ...\")\n",
    "          self.mc_model.load_weights(latest)\n",
    "\n",
    "        print(\"Strat Fitting ...\")\n",
    "        confidences = self.model.predict(self.metric.X)\n",
    "        calibrator = Calibrator(self.calibrator_title)\n",
    "        calibrator.fit(confidences, self.metric.y_real)\n",
    "        print(\"End\")\n",
    "\n",
    "        # print(\"Strat Fitting ...\")\n",
    "        # X_mc = np.empty((len(self.metric.y_real), C), dtype=np.float32)\n",
    "        # y_mc = np.empty((len(self.metric.y_real)), dtype=np.float32)\n",
    "        # X_mc_len = len(X_mc)\n",
    "        # y_mc_len = len(y_mc)\n",
    "        # for i in range(int(self.mc_sn/10)):\n",
    "        #   confidences = self.mc_model(self.metric.X, training=False).numpy()\n",
    "        #   if self.last_activation == 'sigmoid':\n",
    "        #     confidences = softmax(confidences)\n",
    "        #   X_mc = np.concatenate((X_mc, confidences), axis=0)\n",
    "        #   y_mc = np.concatenate((y_mc, self.metric.y_real), axis=0)\n",
    "        # X_mc = X_mc[X_mc_len:]\n",
    "        # y_mc = y_mc[y_mc_len:]\n",
    "        # calibrator = Calibrator(self.calibrator_title)\n",
    "        # calibrator.fit(X_mc, y_mc)\n",
    "        # print(\"End\")\n",
    "\n",
    "        y_prediction, y_calib_prediction, y_mp_prediction, total_u_value_coef, data_u_value_coef, model_u_value_coef, data_u, model_u, total_u = mc_evaulatiobn(\n",
    "            model=self.model, mc_model=self.mc_model,\n",
    "            last_activation=self.last_activation,\n",
    "            calibrator=calibrator, calibrated_uq=True,\n",
    "            X=self.metric.X, mc_sn=self.mc_sn)\n",
    "\n",
    "        value_coef = total_u_value_coef\n",
    "        print(\"value_coef\", np.argwhere(np.isnan(value_coef)))\n",
    "        # print(\"value_coef\", value_coef)\n",
    "        current = self.metric.measurement(y_prediction, value_coef, monitor=self.monitor)\n",
    "        mse = self.metric.measurement(y_prediction, value_coef, monitor=\"ms_mse\")\n",
    "        accuracy = self.metric.measurement(y_prediction, value_coef, monitor=\"ms_accuracy\")\n",
    "        f1 = self.metric.measurement(y_prediction, value_coef, monitor=\"ms_f1\")\n",
    "        logs[self.monitor] = current\n",
    "        logs[self.monitor_method  + \"_uqm_mse\"] = mse\n",
    "        logs[self.monitor_method  + \"_uqm_acc\"] = accuracy\n",
    "        logs[self.monitor_method  + \"_uqm_f1\"] = f1\n",
    "\n",
    "        print('UN epoch %d: [%s: %f] [%s: %f] [%s: %f] [%s: %f]' % (epoch+1, self.monitor, current, \"mse\", mse, \"accuracy\", accuracy, \"f1\", f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIPRTjzc4eLw"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ltka_UrJ4hcy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "def analysis_model(loss_fn, loss_name, y_pred, y_real_raw, segment_size, segment_overlap, decision_size, decision_overlap, value_coef=None):\n",
    "    result = {'Core': {}, 'MV': {}, 'MS': {}}\n",
    "    result['Core']['pesl'] = pesl(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    result['Core']['mse'] = tf.keras.metrics.mean_squared_error(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy().mean()\n",
    "    result['Core']['esl'] = esl(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    result['Core']['ll'] = ll(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                  np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "    result['Core']['accuracy'] = accuracy_score(y_real_raw, y_pred_arg)\n",
    "    result['Core']['precision'] = precision_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['Core']['recall'] = recall_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['Core']['f1'] = f1_score(y_real_raw, y_pred_arg, average='macro')\n",
    "\n",
    "    segments_a_decision_window = get_segments_a_decision_window(segment_size, segment_overlap, decision_size)\n",
    "\n",
    "    # Maximum Score\n",
    "    y_pred_labels, y_real = AveragingProbabilities(y_truth=y_real_raw, y_prediction=y_pred,\n",
    "                                                   s=segments_a_decision_window,\n",
    "                                                   r=decision_overlap,\n",
    "                                                   weights=value_coef)\n",
    "\n",
    "    result['MS']['pesl'] = pesl(y_real, y_pred_labels).numpy()\n",
    "    result['MS']['esl'] = esl(y_real, y_pred_labels).numpy()\n",
    "    result['MS']['ll'] = ll(y_real, y_pred_labels).numpy()\n",
    "    temp = y_pred_labels.copy()\n",
    "    y_pred_labels = np.zeros_like(temp)\n",
    "    y_pred_labels[np.arange(len(temp)), temp.argmax(1)] = 1\n",
    "    result['MS']['accuracy'] = accuracy_score(y_real, y_pred_labels)\n",
    "    result['MS']['precision'] = precision_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MS']['recall'] = recall_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MS']['f1'] = f1_score(y_real, y_pred_labels, average='macro')\n",
    "\n",
    "    # Majority Voting\n",
    "    y_pred_labels, y_real = MajorityVote(y_truth=y_real_raw, y_prediction=y_pred,\n",
    "                                         s=segments_a_decision_window,\n",
    "                                         r=decision_overlap,\n",
    "                                         weights=value_coef)\n",
    "\n",
    "    result['MV']['pesl'] = pesl(y_real, y_pred_labels).numpy()\n",
    "    result['MV']['esl'] = esl(y_real, y_pred_labels).numpy()\n",
    "    result['MV']['ll'] = ll(y_real, y_pred_labels).numpy()\n",
    "\n",
    "    temp = y_pred_labels.copy()\n",
    "    y_pred_labels = np.zeros_like(temp)\n",
    "    y_pred_labels[np.arange(len(temp)), temp.argmax(1)] = 1\n",
    "    result['MV']['accuracy'] = accuracy_score(y_real, y_pred_labels)\n",
    "    result['MV']['precision'] = precision_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MV']['recall'] = recall_score(y_real, y_pred_labels, average='macro')\n",
    "    result['MV']['f1'] = f1_score(y_real, y_pred_labels, average='macro')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZDd-uxHdzY-"
   },
   "outputs": [],
   "source": [
    "def get_segments_a_decision_window(segment_size, segment_overlap, decision_size):\n",
    "    segment_overlap_size = segment_size * segment_overlap\n",
    "    return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJWrMn0Nd5gm"
   },
   "outputs": [],
   "source": [
    "def AveragingProbabilities(y_truth, y_prediction, s, r, weights=None):\n",
    "    t = False\n",
    "    if weights is None:\n",
    "      t = True\n",
    "      weights = np.ones(len(y_truth))\n",
    "    df = []\n",
    "    y_dw_truth = []\n",
    "    c = y_prediction.shape[1]\n",
    "    r = int(np.floor(s * r))\n",
    "    for _id in np.unique(y_truth):\n",
    "        subset = y_prediction[np.where(y_truth == _id)]\n",
    "        weights_sub = weights[np.where(y_truth == _id)]\n",
    "        n = subset.shape[0]\n",
    "        o = int(np.floor((n - r) / (s - r)))\n",
    "        for i in range(o):\n",
    "            row = np.zeros(c)\n",
    "            denominator = 0\n",
    "            cefs = weights_sub[(i * s) : (i * s) + s]\n",
    "            for j in range(s):\n",
    "                row += np.multiply(cefs[j], subset[(i * s) + j])\n",
    "                denominator += cefs[j]\n",
    "            df.append(row / denominator)\n",
    "            y_dw_truth.append(_id)\n",
    "    df = np.array(df)\n",
    "    y_dw_truth = np.asarray(pd.get_dummies(y_dw_truth), dtype=np.int8)\n",
    "    return df, y_dw_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0OxO2RAVd6-D"
   },
   "outputs": [],
   "source": [
    "def MajorityVote(y_truth, y_prediction, s, r, weights=None):\n",
    "    if weights is None:\n",
    "      weights = np.ones(len(y_truth))\n",
    "\n",
    "    df = []\n",
    "    y_dw_truth = []\n",
    "    c = y_prediction.shape[1]\n",
    "    r = int(np.floor(s * r))\n",
    "    # Make prior prediction to one-hot\n",
    "    y_categorical_pred = np.zeros_like(y_prediction)\n",
    "    y_categorical_pred[np.arange(len(y_prediction)), y_prediction.argmax(1)] = 1\n",
    "\n",
    "    for _id in np.unique(y_truth):\n",
    "        subset = y_categorical_pred[np.where(y_truth == _id)]\n",
    "        weights_sub = weights[np.where(y_truth == _id)]\n",
    "        n = subset.shape[0]\n",
    "        o = int(np.floor((n - r) / (s - r)))\n",
    "        for i in range(o):\n",
    "            row = np.zeros(c)\n",
    "            denominator = 0\n",
    "            cefs = weights_sub[(i * s) : (i * s) + s]\n",
    "            for j in range(s):\n",
    "                row += np.multiply(cefs[j], subset[(i * s) + j])\n",
    "                denominator += cefs[j]\n",
    "            df.append(row / denominator)\n",
    "            y_dw_truth.append(_id)\n",
    "    df = np.array(df)\n",
    "    y_dw_truth = np.asarray(pd.get_dummies(y_dw_truth), dtype=np.int8)\n",
    "    return df, y_dw_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmNXTb_cuTSB"
   },
   "source": [
    "# MC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVul5LjBuJ2-"
   },
   "outputs": [],
   "source": [
    "def mc_evaulatiobn(model, mc_model, last_activation, calibrator, calibrated_uq, X, mc_sn=300):\n",
    "        # mc_sn: mont-carlo sampling number\n",
    "\n",
    "        result = []\n",
    "        for i in range(mc_sn):\n",
    "          confidences = model(X, training=True).numpy() #mc_model(X, training=False).numpy()\n",
    "          if calibrated_uq:\n",
    "            confidences = calibrator.transform(confidences)\n",
    "            result.append(confidences)\n",
    "          else:\n",
    "            if last_activation == 'sigmoid':\n",
    "              confidences = softmax(confidences)\n",
    "            result.append(confidences)\n",
    "\n",
    "        result = np.array(result)\n",
    "\n",
    "        predictions = np.mean(result, axis=0)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        # Data Uncertainty\n",
    "        data_u_ent = entropy(result, base=2.0, axis=2)\n",
    "        data_u = np.mean(data_u_ent, axis=0)\n",
    "\n",
    "        # Total Uncertainty\n",
    "        total_u_mu = np.mean(result, axis=0)\n",
    "        total_u = entropy(total_u_mu, base=2.0, axis=1)\n",
    "\n",
    "        # Model Uncertainty\n",
    "        model_u = total_u - data_u\n",
    "\n",
    "        # Value Coefficients\n",
    "        C = predictions.shape[1]\n",
    "        total_u_value_coef = -(1/np.log2(C)) * total_u + 1\n",
    "        data_u_value_coef = -(1/np.log2(C)) * data_u + 1\n",
    "        model_u_value_coef = (1 / np.exp(model_u))**(C-1)\n",
    "\n",
    "        confidences = model.predict(X) #model(X, training=False).numpy()\n",
    "        calibrated_confidences = calibrator.transform(confidences)\n",
    "        if last_activation == 'sigmoid':\n",
    "          confidences = softmax(confidences)\n",
    "\n",
    "        return confidences, calibrated_confidences, predictions, total_u_value_coef, data_u_value_coef, model_u_value_coef, data_u, model_u, total_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_rrgEJu-Am5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def ratio_test(numbers):\n",
    "    difs = []\n",
    "    for i in range(1, len(numbers)):\n",
    "        difs.append(numbers[i] / (numbers[i-1]+sys.float_info.epsilon))\n",
    "    return difs\n",
    "\n",
    "def mc_ratio_test(mc_model, last_activation, calibrator, X, mc_sn=1000):\n",
    "        result = []\n",
    "        uncertainties = []\n",
    "        for i in range(mc_sn):\n",
    "          confidences = mc_model(X, training=True).numpy()\n",
    "          result.append(calibrator.transform(confidences))\n",
    "          #if last_activation == 'sigmoid':\n",
    "          #  confidences = softmax(confidences)\n",
    "          #result.append(confidences)\n",
    "          total_u_mu = np.mean(result, axis=0)\n",
    "          total_u = entropy(total_u_mu, base=2.0, axis=1)\n",
    "          uncertainties.append(total_u)\n",
    "        uncertainties = ratio_test(uncertainties)\n",
    "        uncertainties = np.array(uncertainties)\n",
    "        return uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDN5yNvo4jZ0"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVjqWBgQ4mBR"
   },
   "outputs": [],
   "source": [
    "def train_model(dataset: Dataset, classifier: Classifier, mc_classifier, last_activation, calibrator_title, mc_sn, epochs, batch_size,\n",
    "                log_dir, monitor_metric, monitor_mode, restore_best=True):\n",
    "    callbacks = []\n",
    "    tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    callbacks.append(tbCallBack)\n",
    "\n",
    "    checkpoint_path = log_dir + \"/weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    if restore_best:\n",
    "        metric = ModelAnalyser(segment_size=classifier.segments_size,\n",
    "                               segment_overlap=classifier.segments_overlap,\n",
    "                               decision_size=classifier.decision_size,\n",
    "                               decision_overlap=classifier.decision_overlap,\n",
    "                               X=dataset.X_valid,\n",
    "                               y=dataset.y_valid)\n",
    "        restoring_best_valid = UNLoggingCallback(mc_model=mc_classifier.model,\n",
    "                                                 metric=metric,\n",
    "                                                 monitor=monitor_metric,\n",
    "                                                 mode=monitor_mode,\n",
    "                                                 last_activation=last_activation,\n",
    "                                                 calibrator_title=calibrator_title,\n",
    "                                                 mc_sn=mc_sn,\n",
    "                                                 checkpoint_dir=checkpoint_dir)\n",
    "        callbacks.append(restoring_best_valid)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path, monitor=monitor_metric, verbose=1, save_best_only=True,\n",
    "                                     mode=monitor_mode)\n",
    "        callbacks.append(checkpoint)\n",
    "\n",
    "    print(np.unique(dataset.y_train))\n",
    "    print(np.unique(dataset.y_valid))\n",
    "    print(np.unique(dataset.y_test))\n",
    "\n",
    "    history = classifier.train(epochs=epochs,\n",
    "                               X_train=dataset.X_train,\n",
    "                               y_train=dataset.y_train,\n",
    "                               X_valid=dataset.X_valid,\n",
    "                               y_valid=dataset.y_valid,\n",
    "                               batch_size=batch_size,\n",
    "                               callback=callbacks,\n",
    "                               monitor_mode=monitor_mode)\n",
    "\n",
    "    # Train Calibrator with Validation Data\n",
    "    print(\"Strat Fitting ...\")\n",
    "    confidences = classifier.model.predict(dataset.X_valid)\n",
    "    calibrator = Calibrator(calibrator_title)\n",
    "    calibrator.fit(confidences, dataset.y_valid)\n",
    "    print(\"End\")\n",
    "\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest != None:\n",
    "      print(\"loading weights to mc model ...\")\n",
    "      mc_classifier.model.load_weights(latest)\n",
    "\n",
    "    # print(\"Strat Fitting ...\")\n",
    "    # C = len(np.unique(dataset.y_valid))\n",
    "\n",
    "    # X_mc = np.empty((len(dataset.y_valid), C), dtype=np.float32)\n",
    "    # y_mc = np.empty((len(dataset.y_valid)), dtype=np.float32)\n",
    "    # X_mc_len = len(X_mc)\n",
    "    # y_mc_len = len(y_mc)\n",
    "    # for i in range(mc_sn):\n",
    "    #   confidences = mc_classifier.model(dataset.X_valid, training=False).numpy()\n",
    "    #   if last_activation == 'sigmoid':\n",
    "    #     confidences = softmax(confidences)\n",
    "    #   X_mc = np.concatenate((X_mc, confidences), axis=0)\n",
    "    #   y_mc = np.concatenate((y_mc, dataset.y_valid), axis=0)\n",
    "    # X_mc = X_mc[X_mc_len:]\n",
    "    # y_mc = y_mc[y_mc_len:]\n",
    "    # calibrator = Calibrator(calibrator_title)\n",
    "    # calibrator.fit(X_mc, y_mc)\n",
    "    # print(\"End\")\n",
    "\n",
    "    # Draw Reliability Diagram\n",
    "    # print(\"diagram\")\n",
    "    # diagram = ReliabilityDiagram(20)\n",
    "    # diagram.plot(confidences, dataset.y_valid, tikz=True, filename=\"confidences.tikz\")\n",
    "    # diagram.plot(calibrator.transform(confidences), dataset.y_valid, tikz=True, filename=\"calibrated.tikz\")\n",
    "    # End\n",
    "\n",
    "    # Monte-Carlo Ratio Test\n",
    "    valid_ratio_test = mc_ratio_test(mc_classifier.model, last_activation, calibrator, dataset.X_test, mc_sn=1000)\n",
    "\n",
    "    # Monte-Carlo Evaluation: With Calibrator\n",
    "    y_test_pred, y_test_calib_pred, mc_mean_pred, total_u_value_coef, data_u_value_coef, model_u_value_coef, data_u, model_u, total_u = mc_evaulatiobn(\n",
    "        classifier.model, mc_classifier.model, last_activation, calibrator, True, dataset.X_test, mc_sn)\n",
    "    print(\"UQ1\", data_u, model_u, total_u)\n",
    "\n",
    "    # Monte-Carlo Evaluation: Without Calibrator\n",
    "    y_test_pred1, y_test_calib_pred1, mc_mean_pred1, total_u_value_coef1, data_u_value_coef1, model_u_value_coef1, data_u1, model_u1, total_u1 = mc_evaulatiobn(\n",
    "        classifier.model, mc_classifier.model, last_activation, calibrator, False, dataset.X_test, mc_sn)\n",
    "    print(\"UQ2\", data_u1, model_u1, total_u1)\n",
    "\n",
    "    # Statistics After Fusion With Calibrator\n",
    "    tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test = final_evaluation(\n",
    "        classifier, y_test_pred, y_test_calib_pred, dataset.y_test, total_u_value_coef, data_u_value_coef, model_u_value_coef)\n",
    "    print(\"UQ with Calibrator\")\n",
    "    print_results(result_test, calib_result_test, tm_org_result_test, tm_result_test)\n",
    "\n",
    "    # Statistics After Fusion Without Calibrator\n",
    "    tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1 = final_evaluation(\n",
    "        classifier, y_test_pred1, y_test_calib_pred1, dataset.y_test, total_u_value_coef1, data_u_value_coef1, model_u_value_coef1)\n",
    "    print(\"UQ without Calibrator\")\n",
    "    print_results(result_test1, calib_result_test1, tm_org_result_test1, tm_result_test1)\n",
    "\n",
    "    # Add UQ Metric to Results\n",
    "    tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test = add_uq_to_results(\n",
    "        np.mean(total_u), np.mean(model_u), np.mean(data_u), tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test)\n",
    "    tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1 = add_uq_to_results(\n",
    "        np.mean(total_u1), np.mean(model_u1), np.mean(data_u1), tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1)\n",
    "    # print(\"Uncertainty With Calibrator\", np.mean(data_u), np.mean(model_u), np.mean(total_u))\n",
    "    # print(\"Uncertainty Without Calibrator\", np.mean(data_u1), np.mean(model_u1), np.mean(total_u1))\n",
    "\n",
    "    # Add Validation Monitor Metric to Results\n",
    "    tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test = add_validation_monitor_metric_to_results(\n",
    "        history, monitor_mode, monitor_metric,\n",
    "        tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test)\n",
    "    tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1 = add_validation_monitor_metric_to_results(\n",
    "        history, monitor_mode, monitor_metric,\n",
    "        tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1)\n",
    "\n",
    "    # Add CALIBRATION Metrics\n",
    "    tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test = add_calibration_metrics_to_results(\n",
    "        y_test_pred, y_test_calib_pred, dataset.y_test,\n",
    "        tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test)\n",
    "    tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1 = add_calibration_metrics_to_results(\n",
    "        y_test_pred1, y_test_calib_pred1, dataset.y_test,\n",
    "        tm_org_result_test1, tm_result_test1, dm_result_test1, mm_result_test1, calib_result_test1, result_test1)\n",
    "\n",
    "    result = {}\n",
    "    result['InCls:[UnCalibrated]_UQ:[Calibrated][Totall]'] = tm_org_result_test\n",
    "    result['InCls:[UnCalibrated]_UQ:[UnCalibrated][Totall]'] = tm_org_result_test1\n",
    "\n",
    "    result['InCls:[Calibrated]_UQ:[Calibrated][Totall]'] = tm_result_test\n",
    "    result['InCls:[Calibrated]_UQ:[UnCalibrated][Totall]'] = tm_result_test1\n",
    "\n",
    "    result['InCls:[Calibrated]_UQ:[Calibrated][Data]'] = dm_result_test\n",
    "    result['InCls:[Calibrated]_UQ:[UnCalibrated][Data]'] = dm_result_test1\n",
    "\n",
    "    result['InCls:[Calibrated]_UQ:[Calibrated][Model]'] = mm_result_test\n",
    "    result['InCls:[Calibrated]_UQ:[UnCalibrated][Model]'] = mm_result_test1\n",
    "\n",
    "    result['InCls:[Calibrated]_UQ:[-][No]'] = calib_result_test\n",
    "    result['InCls:[Calibrated]_UQ:[-][No]'] = calib_result_test1\n",
    "\n",
    "    result['InCls:[UnCalibrated]_UQ:[-][No]'] = result_test\n",
    "    result['InCls:[UnCalibrated]_UQ:[-][No]'] = result_test1\n",
    "\n",
    "    return result, valid_ratio_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PUQLt_277Js"
   },
   "outputs": [],
   "source": [
    "def final_evaluation(classifier, y_test_pred, y_test_calib_pred, y_real_raw, total_u_value_coef, data_u_value_coef, model_u_value_coef):\n",
    "\n",
    "    tm_org_result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                        loss_name=classifier.get_loss_function_name(),\n",
    "                                        y_pred=y_test_pred,\n",
    "                                        value_coef=total_u_value_coef,\n",
    "                                        y_real_raw=y_real_raw,\n",
    "                                        segment_size=classifier.segments_size,\n",
    "                                        segment_overlap=classifier.segments_overlap,\n",
    "                                        decision_size=classifier.decision_size,\n",
    "                                        decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    tm_result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                    loss_name=classifier.get_loss_function_name(),\n",
    "                                    y_pred=y_test_calib_pred,\n",
    "                                    value_coef=total_u_value_coef,\n",
    "                                    y_real_raw=y_real_raw,\n",
    "                                    segment_size=classifier.segments_size,\n",
    "                                    segment_overlap=classifier.segments_overlap,\n",
    "                                    decision_size=classifier.decision_size,\n",
    "                                    decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    dm_result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                    loss_name=classifier.get_loss_function_name(),\n",
    "                                    y_pred=y_test_calib_pred,\n",
    "                                    value_coef=data_u_value_coef,\n",
    "                                    y_real_raw=y_real_raw,\n",
    "                                    segment_size=classifier.segments_size,\n",
    "                                    segment_overlap=classifier.segments_overlap,\n",
    "                                    decision_size=classifier.decision_size,\n",
    "                                    decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    mm_result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                    loss_name=classifier.get_loss_function_name(),\n",
    "                                    y_pred=y_test_calib_pred,\n",
    "                                    value_coef=model_u_value_coef,\n",
    "                                    y_real_raw=y_real_raw,\n",
    "                                    segment_size=classifier.segments_size,\n",
    "                                    segment_overlap=classifier.segments_overlap,\n",
    "                                    decision_size=classifier.decision_size,\n",
    "                                    decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    calib_result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                       loss_name=classifier.get_loss_function_name(),\n",
    "                                       y_pred=y_test_calib_pred,\n",
    "                                       value_coef=None,\n",
    "                                       y_real_raw=y_real_raw,\n",
    "                                       segment_size=classifier.segments_size,\n",
    "                                       segment_overlap=classifier.segments_overlap,\n",
    "                                       decision_size=classifier.decision_size,\n",
    "                                       decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                 loss_name=classifier.get_loss_function_name(),\n",
    "                                 y_pred=y_test_pred,\n",
    "                                 value_coef=None,\n",
    "                                 y_real_raw=y_real_raw,\n",
    "                                 segment_size=classifier.segments_size,\n",
    "                                 segment_overlap=classifier.segments_overlap,\n",
    "                                 decision_size=classifier.decision_size,\n",
    "                                 decision_overlap=classifier.decision_overlap)\n",
    "\n",
    "    return tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIAK2SUuANIg"
   },
   "outputs": [],
   "source": [
    "def print_results(result_test, calib_result_test, tm_org_result_test, tm_result_test):\n",
    "    print('OR Results: Classifier:%s Test(Core):%5.4f Test(MS)/acc:%5.4f Test(MS)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, result_test['Core']['accuracy'], result_test['MS']['accuracy'], result_test['MS']['f1']))\n",
    "    print('CB Results: Classifier:%s Test(Core):%5.4f Test(MS)/acc:%5.4f Test(MS)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, calib_result_test['Core']['accuracy'], calib_result_test['MS']['accuracy'],\n",
    "    calib_result_test['MS']['f1']))\n",
    "    print('UN Results: Classifier:%s Test(Core):%5.4f Test(MS)/acc:%5.4f Test(MS)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, tm_org_result_test['Core']['accuracy'], tm_org_result_test['MS']['accuracy'],\n",
    "    tm_org_result_test['MS']['f1']))\n",
    "    print('UC Results: Classifier:%s Test(Core):%5.4f Test(MS)/acc:%5.4f Test(MS)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, tm_result_test['Core']['accuracy'], tm_result_test['MS']['accuracy'],\n",
    "    tm_result_test['MS']['f1']))\n",
    "\n",
    "    print('OR Results: Classifier:%s Test(Core):%5.4f Test(MV)/acc:%5.4f Test(MV)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, result_test['Core']['accuracy'], result_test['MV']['accuracy'], result_test['MV']['f1']))\n",
    "    print('CB Results: Classifier:%s Test(Core):%5.4f Test(MV)/acc:%5.4f Test(MV)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, calib_result_test['Core']['accuracy'], calib_result_test['MV']['accuracy'],\n",
    "    calib_result_test['MV']['f1']))\n",
    "    print('UN Results: Classifier:%s Test(Core):%5.4f Test(MV)/acc:%5.4f Test(MV)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, tm_org_result_test['Core']['accuracy'], tm_org_result_test['MV']['accuracy'],\n",
    "    tm_org_result_test['MV']['f1']))\n",
    "    print('UC Results: Classifier:%s Test(Core):%5.4f Test(MV)/acc:%5.4f Test(MV)/f1:%5.4f' % (\n",
    "    type(classifier).__name__, tm_result_test['Core']['accuracy'], tm_result_test['MV']['accuracy'],\n",
    "    tm_result_test['MV']['f1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw5v9E9CBBWI"
   },
   "outputs": [],
   "source": [
    "def add_validation_monitor_metric_to_results(history, monitor_mode, monitor_metric, tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test):\n",
    "    tm_org_result_test['validation_metric'] = {}\n",
    "    tm_result_test['validation_metric'] = {}\n",
    "    dm_result_test['validation_metric'] = {}\n",
    "    mm_result_test['validation_metric'] = {}\n",
    "    calib_result_test['validation_metric'] = {}\n",
    "    result_test['validation_metric'] = {}\n",
    "\n",
    "    if monitor_mode == 'min':\n",
    "        tm_org_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        tm_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        dm_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        mm_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        calib_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "    if monitor_mode == 'max':\n",
    "        tm_org_result_test['validation_metric'][monitor_metric] = np.max(history.history[monitor_metric])\n",
    "        tm_result_test['validation_metric'][monitor_metric] = np.max(history.history[monitor_metric])\n",
    "        dm_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        mm_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        calib_result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "        result_test['validation_metric'][monitor_metric] = np.min(history.history[monitor_metric])\n",
    "\n",
    "    return tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H1sahUoYHULg"
   },
   "outputs": [],
   "source": [
    "def add_calibration_metrics_to_results(y_test_pred, y_test_calib_pred, y_real_raw, tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test):\n",
    "    tm_org_result_test['calibration_metric'] = {}\n",
    "    tm_result_test['calibration_metric'] = {}\n",
    "    dm_result_test['calibration_metric'] = {}\n",
    "    mm_result_test['calibration_metric'] = {}\n",
    "    calib_result_test['calibration_metric'] = {}\n",
    "    result_test['calibration_metric'] = {}\n",
    "\n",
    "    ece = ECE(20)\n",
    "    uncalibrated_score = ece.measure(y_test_pred, y_real_raw)\n",
    "    calibrated_score = ece.measure(y_test_calib_pred, y_real_raw)\n",
    "    tm_org_result_test['calibration_metric']['ECE'] = uncalibrated_score\n",
    "    tm_result_test['calibration_metric']['ECE'] = calibrated_score\n",
    "    dm_result_test['calibration_metric']['ECE'] = calibrated_score\n",
    "    mm_result_test['calibration_metric']['ECE'] = calibrated_score\n",
    "    calib_result_test['calibration_metric']['ECE'] = calibrated_score\n",
    "    result_test['calibration_metric']['ECE'] = uncalibrated_score\n",
    "\n",
    "    print('Calibration Erorr: befor %5.4f after f1:%5.4f' % (uncalibrated_score, calibrated_score))\n",
    "\n",
    "    ece = ACE(20)\n",
    "    uncalibrated_score = ece.measure(y_test_pred, y_real_raw)\n",
    "    calibrated_score = ece.measure(y_test_calib_pred, y_real_raw)\n",
    "    tm_org_result_test['calibration_metric']['ACE'] = uncalibrated_score\n",
    "    tm_result_test['calibration_metric']['ACE'] = calibrated_score\n",
    "    dm_result_test['calibration_metric']['ACE'] = calibrated_score\n",
    "    mm_result_test['calibration_metric']['ACE'] = calibrated_score\n",
    "    calib_result_test['calibration_metric']['ACE'] = calibrated_score\n",
    "    result_test['calibration_metric']['ACE'] = uncalibrated_score\n",
    "\n",
    "    ece = MCE(20)\n",
    "    uncalibrated_score = ece.measure(y_test_pred, y_real_raw)\n",
    "    calibrated_score = ece.measure(y_test_calib_pred, y_real_raw)\n",
    "    tm_org_result_test['calibration_metric']['MCE'] = uncalibrated_score\n",
    "    tm_result_test['calibration_metric']['MCE'] = calibrated_score\n",
    "    dm_result_test['calibration_metric']['MCE'] = calibrated_score\n",
    "    mm_result_test['calibration_metric']['MCE'] = calibrated_score\n",
    "    calib_result_test['calibration_metric']['MCE'] = calibrated_score\n",
    "    result_test['calibration_metric']['MCE'] = uncalibrated_score\n",
    "\n",
    "    #ece = MMCE()\n",
    "    #uncalibrated_score = ece.measure(y_test_pred, y_real_raw)\n",
    "    #calibrated_score = ece.measure(y_test_calib_pred, y_real_raw)\n",
    "    #tm_org_result_test['calibration_metric']['MMCE'] = uncalibrated_score\n",
    "    #tm_result_test['calibration_metric']['MMCE'] = calibrated_score\n",
    "    #dm_result_test['calibration_metric']['MMCE'] = calibrated_score\n",
    "    #mm_result_test['calibration_metric']['MMCE'] = calibrated_score\n",
    "    #calib_result_test['calibration_metric']['MMCE'] = calibrated_score\n",
    "    #result_test['calibration_metric']['MMCE'] = uncalibrated_score\n",
    "\n",
    "    return tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cpd3HWbCXl5"
   },
   "outputs": [],
   "source": [
    "def add_uq_to_results(total, model, data, tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test):\n",
    "    tm_org_result_test['UQ'] = {}\n",
    "    tm_result_test['UQ'] = {}\n",
    "    dm_result_test['UQ'] = {}\n",
    "    mm_result_test['UQ'] = {}\n",
    "    calib_result_test['UQ'] = {}\n",
    "    result_test['UQ'] = {}\n",
    "\n",
    "    tm_org_result_test['UQ']['Total'] = total\n",
    "    tm_result_test['UQ']['Total'] = total\n",
    "    dm_result_test['UQ']['Total'] = total\n",
    "    mm_result_test['UQ']['Total'] = total\n",
    "    calib_result_test['UQ']['Total'] = total\n",
    "    result_test['UQ']['Total'] = total\n",
    "\n",
    "    tm_org_result_test['UQ']['Model'] = model\n",
    "    tm_result_test['UQ']['Model'] = model\n",
    "    dm_result_test['UQ']['Model'] = model\n",
    "    mm_result_test['UQ']['Model'] = model\n",
    "    calib_result_test['UQ']['Model'] = model\n",
    "    result_test['UQ']['Model'] = model\n",
    "\n",
    "    tm_org_result_test['UQ']['Data'] = data\n",
    "    tm_result_test['UQ']['Data'] = data\n",
    "    dm_result_test['UQ']['Data'] = data\n",
    "    mm_result_test['UQ']['Data'] = data\n",
    "    calib_result_test['UQ']['Data'] = data\n",
    "    result_test['UQ']['Data'] = data\n",
    "\n",
    "    return tm_org_result_test, tm_result_test, dm_result_test, mm_result_test, calib_result_test, result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9CZOYRH4y8U"
   },
   "source": [
    "# Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lX06Xc3o4zQw"
   },
   "outputs": [],
   "source": [
    "def h_block_analyzer(db_path, sample_rate, features, n_classes, noise_rate, label_noise_rate, segments_time,\n",
    "                     segments_overlap, decision_time, decision_overlap, classifier, mc_classifier, last_activation, calibrator_title, mc_sn, epochs, batch_size,\n",
    "                     data_length_time, monitor_metric, monitor_mode, restore_best, repetitions, n_h_block,\n",
    "                     n_train_h_block, n_valid_h_block, n_test_h_block, h_moving_step=1):\n",
    "    \"\"\"\n",
    "    :param db_path: the address of dataset directory\n",
    "    :param sample_rate: the sampling rate of signals\n",
    "    :param features: the signals of original data\n",
    "    :param n_classes: the number of classes\n",
    "    :param noise_rate: the rate of noises injected to test data\n",
    "    :param segments_time: the length of each segment in seconds.\n",
    "    :param segments_overlap: the overlap of each segment\n",
    "    :param classifier: the inner classifier\n",
    "    :param calibrator_title: the calibration model\n",
    "    :param epochs: the number of training epochs\n",
    "    :param batch_size: the number of segments in each batch\n",
    "    :param data_length_time: the length of data of each class. -1 = whole.\n",
    "    :param n_h_block: the number of all hv blocks\n",
    "    :param n_train_h_block: the number of hv blocks to train network\n",
    "    :param n_valid_h_block: the number of hv blocks to validate network\n",
    "    :param n_test_h_block: the number of hv blocks to test network\n",
    "    :param h_moving_step: the number of movement of test and validation blocks in each iteration\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    add_noise = noise_rate < 100\n",
    "\n",
    "    # Create hv blocks\n",
    "    data_blocks = [i for i in range(n_h_block)]\n",
    "    np.random.shuffle(data_blocks)\n",
    "    n_vt = (n_valid_h_block + n_test_h_block)\n",
    "    n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    final_statistics = {}\n",
    "    valid_ratio_tests = []\n",
    "    for i in range(n_iteration + 1):\n",
    "        i = 1\n",
    "        i_statistics = {}\n",
    "        for j in range(repetitions):\n",
    "            print('Iteration:  %d/%d +++++++++++++++++++++++++++++++++++++++++++++++++' % (i + 1, n_iteration + 1))\n",
    "            print('Repetition: %d/%d -------------------------------------------------' % (j + 1, repetitions))\n",
    "\n",
    "            training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "            train_blocks = training_container[:n_train_h_block]\n",
    "            valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "            test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "            dataset = Dataset(db_path,\n",
    "                              sample_rate,\n",
    "                              features=features,\n",
    "                              window_time=segments_time,\n",
    "                              window_overlap_percentage=segments_overlap,\n",
    "                              decision_time=decision_time,\n",
    "                              decision_overlap_percentage=decision_overlap,\n",
    "                              add_noise=add_noise,\n",
    "                              noise_rate=noise_rate,\n",
    "                              label_noise_rate=label_noise_rate,\n",
    "                              train_blocks=train_blocks,\n",
    "                              valid_blocks=valid_blocks,\n",
    "                              test_blocks=test_blocks,\n",
    "                              data_length_time=data_length_time)\n",
    "            dataset.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "            print(dataset.X_train.shape)\n",
    "            print(dataset.y_train.shape)\n",
    "\n",
    "            logdir = os.path.join(\"logs/checkpointss/\", date_str + \"[\" + str(i) + \"]/[\" + str(j) + \"]\")\n",
    "            if not os.path.exists(logdir):\n",
    "                os.makedirs(logdir)\n",
    "\n",
    "            result, valid_ratio_test = train_model(dataset=dataset,\n",
    "                                                   classifier=classifier,\n",
    "                                                   mc_classifier=mc_classifier,\n",
    "                                                   calibrator_title=calibrator_title,\n",
    "                                                   last_activation=last_activation,\n",
    "                                                   mc_sn=mc_sn,\n",
    "                                                   epochs=epochs,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   restore_best=restore_best,\n",
    "                                                   monitor_metric=monitor_metric,\n",
    "                                                   monitor_mode=monitor_mode,\n",
    "                                                   log_dir=logdir)\n",
    "\n",
    "            valid_ratio_tests.append(valid_ratio_test)\n",
    "\n",
    "            for prediction_method in result.keys():\n",
    "                if not prediction_method in i_statistics:\n",
    "                  i_statistics[prediction_method] = {}\n",
    "                for key in result[prediction_method].keys():\n",
    "                    if not key in i_statistics[prediction_method]:\n",
    "                        i_statistics[prediction_method][key] = {}\n",
    "                    for inner_key in result[prediction_method][key].keys():\n",
    "                        if not inner_key in i_statistics[prediction_method][key]:\n",
    "                            i_statistics[prediction_method][key][inner_key] = []\n",
    "                        i_statistics[prediction_method][key][inner_key].append(result[prediction_method][key][inner_key])\n",
    "            shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "        for prediction_method in i_statistics.keys():\n",
    "            if not prediction_method in final_statistics:\n",
    "                final_statistics[prediction_method] = {}\n",
    "            for key in i_statistics[prediction_method].keys():\n",
    "                if not key in final_statistics[prediction_method]:\n",
    "                    final_statistics[prediction_method][key] = {}\n",
    "                for inner_key in i_statistics[prediction_method][key].keys():\n",
    "                    if not inner_key in final_statistics[prediction_method][key]:\n",
    "                      final_statistics[prediction_method][key][inner_key] = []\n",
    "\n",
    "                    if monitor_mode == \"max\":\n",
    "                      selected_index = np.nanargmax(i_statistics[prediction_method]['validation_metric'][monitor_metric])\n",
    "                    else:\n",
    "                      selected_index = np.nanargmin(i_statistics[prediction_method]['validation_metric'][monitor_metric])\n",
    "                    final_statistics[prediction_method][key][inner_key].append(i_statistics[prediction_method][key][inner_key][selected_index])\n",
    "\n",
    "    # print(\"final_statistics\", final_statistics)\n",
    "    return final_statistics, valid_ratio_tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TtSkS-W454c"
   },
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1mJTX0Y46OV"
   },
   "outputs": [],
   "source": [
    "def save_result(log_dir, data: dict):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Save to file\n",
    "    with open(log_dir + 'statistics.txt', 'a') as f:\n",
    "        f.write('\\n==========***==========\\n')\n",
    "        f.write(str(data))\n",
    "        f.write('\\n')\n",
    "\n",
    "    csv_file = log_dir + 'statistics.csv'\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    try:\n",
    "        with open(csv_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not file_exists:\n",
    "                writer.writerow(data.keys())\n",
    "            writer.writerow(data.values())\n",
    "            csvfile.close()\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Whr7yWQL48po"
   },
   "outputs": [],
   "source": [
    "def cumulative_average(numbers):\n",
    "    avg = []\n",
    "    for i in range(len(numbers)):\n",
    "        check = 0\n",
    "        l = i + 1\n",
    "        for j in range(i+1):\n",
    "            check += numbers[j]\n",
    "        avg.append(check/l)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oo0q8aqH4-4w"
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzgHB2zC5Bk_"
   },
   "outputs": [],
   "source": [
    "problems = {'ConfLongDemo_JSI':{},\n",
    "            'Healthy_Older_People':{},\n",
    "            'User_Identification_From_Walking':{},\n",
    "            'Dataset#2':{},\n",
    "            'UCI-HAR-Dataset':{},\n",
    "           }\n",
    "\n",
    "problems['ConfLongDemo_JSI']['dataset'] = './datasets/ConfLongDemo_JSI/'\n",
    "problems['ConfLongDemo_JSI']['n_classes'] = 5\n",
    "problems['ConfLongDemo_JSI']['features'] = [\"x\", \"y\", \"z\"]\n",
    "problems['ConfLongDemo_JSI']['sample_rate'] = 30\n",
    "problems['ConfLongDemo_JSI']['data_length_time'] = -1\n",
    "problems['ConfLongDemo_JSI']['n_h_block'] = 10\n",
    "problems['ConfLongDemo_JSI']['n_train_h_block'] = 5\n",
    "problems['ConfLongDemo_JSI']['n_valid_h_block'] = 2\n",
    "problems['ConfLongDemo_JSI']['n_test_h_block'] = 3\n",
    "problems['ConfLongDemo_JSI']['h_moving_step'] = 1\n",
    "problems['ConfLongDemo_JSI']['segments_overlaps'] = 0.75\n",
    "problems['ConfLongDemo_JSI']['MLP/segments_time'] = 3\n",
    "problems['ConfLongDemo_JSI']['CNN_L/segments_time'] = 3\n",
    "problems['ConfLongDemo_JSI']['decision_overlaps'] = 0\n",
    "problems['ConfLongDemo_JSI']['MLP/decision_times'] = 2 * 60\n",
    "problems['ConfLongDemo_JSI']['CNN_L/decision_times'] = 2 * 60\n",
    "problems['ConfLongDemo_JSI']['M'] = 20\n",
    "\n",
    "problems['Healthy_Older_People']['dataset'] = './datasets/Healthy_Older_People/'\n",
    "problems['Healthy_Older_People']['n_classes'] = 12\n",
    "problems['Healthy_Older_People']['features'] = [\"X\", \"Y\", \"Z\"]\n",
    "problems['Healthy_Older_People']['sample_rate'] = 1\n",
    "problems['Healthy_Older_People']['data_length_time'] = -1\n",
    "problems['Healthy_Older_People']['n_h_block'] = 10\n",
    "problems['Healthy_Older_People']['n_train_h_block'] = 5\n",
    "problems['Healthy_Older_People']['n_valid_h_block'] = 2\n",
    "problems['Healthy_Older_People']['n_test_h_block'] = 3\n",
    "problems['Healthy_Older_People']['h_moving_step'] = 1\n",
    "problems['Healthy_Older_People']['segments_overlaps'] = 0.75\n",
    "problems['Healthy_Older_People']['MLP/segments_time'] = 8\n",
    "problems['Healthy_Older_People']['CNN_L/segments_time'] = 8\n",
    "problems['Healthy_Older_People']['decision_overlaps'] = 0\n",
    "problems['Healthy_Older_People']['MLP/decision_times'] = 3 * 60\n",
    "problems['Healthy_Older_People']['CNN_L/decision_times'] = 3 * 60\n",
    "problems['Healthy_Older_People']['M'] = 30\n",
    "\n",
    "problems['User_Identification_From_Walking']['dataset'] = './datasets/User_Identification_From_Walking/'\n",
    "problems['User_Identification_From_Walking']['n_classes'] = 13\n",
    "problems['User_Identification_From_Walking']['features'] = [' x acceleration', ' y acceleration', ' z acceleration']\n",
    "problems['User_Identification_From_Walking']['sample_rate'] = 32\n",
    "problems['User_Identification_From_Walking']['data_length_time'] = -1\n",
    "problems['User_Identification_From_Walking']['n_h_block'] = 10\n",
    "problems['User_Identification_From_Walking']['n_train_h_block'] = 5\n",
    "problems['User_Identification_From_Walking']['n_valid_h_block'] = 2\n",
    "problems['User_Identification_From_Walking']['n_test_h_block'] = 3\n",
    "problems['User_Identification_From_Walking']['h_moving_step'] = 1\n",
    "problems['User_Identification_From_Walking']['segments_overlaps'] = 0.75\n",
    "problems['User_Identification_From_Walking']['MLP/segments_time'] = 3\n",
    "problems['User_Identification_From_Walking']['CNN_L/segments_time'] = 3\n",
    "problems['User_Identification_From_Walking']['decision_overlaps'] = 0\n",
    "problems['User_Identification_From_Walking']['MLP/decision_times'] = 22\n",
    "problems['User_Identification_From_Walking']['CNN_L/decision_times'] = 22\n",
    "problems['User_Identification_From_Walking']['M'] = 30\n",
    "\n",
    "problems['Dataset#2']['dataset'] = './datasets/Dataset#2/'\n",
    "problems['Dataset#2']['n_classes'] = 20\n",
    "problems['Dataset#2']['features'] = ['x1','y1','z1','x2','y2','z2']\n",
    "problems['Dataset#2']['sample_rate'] = 1\n",
    "problems['Dataset#2']['data_length_time'] = -1\n",
    "problems['Dataset#2']['n_h_block'] = 2\n",
    "problems['Dataset#2']['n_train_h_block'] = 1\n",
    "problems['Dataset#2']['n_valid_h_block'] = 1\n",
    "problems['Dataset#2']['n_test_h_block'] = 1\n",
    "problems['Dataset#2']['h_moving_step'] = 1\n",
    "problems['Dataset#2']['segments_overlaps'] = 0.75\n",
    "problems['Dataset#2']['CNN_L/segments_time'] = 32\n",
    "problems['Dataset#2']['decision_overlaps'] = 0\n",
    "problems['Dataset#2']['CNN_L/decision_times'] = 128\n",
    "problems['Dataset#2']['M'] = 20\n",
    "\n",
    "problems['UCI-HAR-Dataset']['dataset'] = './datasets/UCI-HAR-Dataset/'\n",
    "problems['UCI-HAR-Dataset']['n_classes'] = 30\n",
    "problems['UCI-HAR-Dataset']['features'] = ['x1','y1','z1','x2','y2','z2','x3','y3','z3']\n",
    "problems['UCI-HAR-Dataset']['sample_rate'] = 1\n",
    "problems['UCI-HAR-Dataset']['data_length_time'] = -1\n",
    "problems['UCI-HAR-Dataset']['n_h_block'] = 2\n",
    "problems['UCI-HAR-Dataset']['n_train_h_block'] = 1\n",
    "problems['UCI-HAR-Dataset']['n_valid_h_block'] = 1\n",
    "problems['UCI-HAR-Dataset']['n_test_h_block'] = 1\n",
    "problems['UCI-HAR-Dataset']['h_moving_step'] = 1\n",
    "problems['UCI-HAR-Dataset']['segments_overlaps'] = 0.75\n",
    "problems['UCI-HAR-Dataset']['CNN_L/segments_time'] = 32\n",
    "problems['UCI-HAR-Dataset']['decision_overlaps'] = 0\n",
    "problems['UCI-HAR-Dataset']['CNN_L/decision_times'] = 128\n",
    "problems['UCI-HAR-Dataset']['M'] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFsacgLZ5EG4"
   },
   "source": [
    "# Main Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O8nfqyGV5GzQ"
   },
   "outputs": [],
   "source": [
    "train_config = [\n",
    "    {\n",
    "        \"loss_function\": tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        \"loss_metric\": pesl,\n",
    "        \"monitor_metric\": \"ms_pesl\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "        \"loss_metric\": pll,\n",
    "        \"monitor_metric\": \"ms_pll\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EcygQRU5SK_"
   },
   "source": [
    "# Models Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTcORg6Ci4SY"
   },
   "outputs": [],
   "source": [
    "valid_ratio_tests_dic = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WRP2mSzx5U2I",
    "outputId": "242cf7ef-e70d-4318-93bb-d7c98974605d"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'problems' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m problem \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m     19\u001b[0m     log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./log/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mproblems\u001b[49m[problem][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m problems[problem][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m     features \u001b[38;5;241m=\u001b[39m problems[problem][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'problems' is not defined"
     ]
    }
   ],
   "source": [
    "noise_rate = 100\n",
    "label_noise_rate = 0.0\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "models = ['CNN_L']\n",
    "\n",
    "# calibration_models : 'HistogramBinning', 'IsotonicRegression', 'ENIR', 'BBQ', 'LogisticCalibration', 'TemperatureScaling', 'BetaCalibration'\n",
    "calibration_models = ['HistogramBinning', 'IsotonicRegression', 'ENIR', 'BBQ', 'LogisticCalibration', 'TemperatureScaling', 'BetaCalibration']\n",
    "repetitions = 10\n",
    "restore_best = True\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "\n",
    "# 'ConfLongDemo_JSI', 'Healthy_Older_People', 'User_Identification_From_Walking'\n",
    "datasets = ['ConfLongDemo_JSI', 'Healthy_Older_People', 'User_Identification_From_Walking']\n",
    "for model in models:\n",
    "    for calibrator_title in calibration_models:\n",
    "        for problem in datasets:\n",
    "            log_dir = f\"./log/{problem}/\"\n",
    "            dataset = problems[problem]['dataset']\n",
    "            n_classes = problems[problem]['n_classes']\n",
    "            features = problems[problem]['features']\n",
    "            sample_rate = problems[problem]['sample_rate']\n",
    "            data_length_time = problems[problem]['data_length_time']\n",
    "            n_h_block = problems[problem]['n_h_block']\n",
    "            n_train_h_block = problems[problem]['n_train_h_block']\n",
    "            n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "            n_test_h_block = problems[problem]['n_test_h_block']\n",
    "            h_moving_step = problems[problem]['h_moving_step']\n",
    "            # train config index\n",
    "            for tci in [0]:\n",
    "                if calibrator_title in ['HistogramBinning', 'IsotonicRegression', 'ENIR', 'BBQ']:\n",
    "                  last_activation = \"softmax\"\n",
    "                  train_config[tci][\"loss_function\"].from_logits = False\n",
    "                else:\n",
    "                  last_activation = \"softmax\"\n",
    "                  train_config[tci][\"loss_function\"].from_logits = False\n",
    "                segments_time = problems[problem][model + '/segments_time']\n",
    "                segments_overlap = problems[problem]['segments_overlaps']\n",
    "                decision_time = problems[problem][model + '/decision_times']\n",
    "                decision_overlap = problems[problem]['decision_overlaps']\n",
    "                classifier = eval(model)(classes=n_classes,\n",
    "                                         n_features=len(features),\n",
    "                                         segments_size=int(segments_time * sample_rate),\n",
    "                                         segments_overlap=segments_overlap,\n",
    "                                         decision_size=int(decision_time * sample_rate),\n",
    "                                         decision_overlap=decision_overlap,\n",
    "                                         loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                         loss_function=train_config[tci][\"loss_function\"],\n",
    "                                         lr=lr,\n",
    "                                         beta_1=beta_1,\n",
    "                                         last_activation=last_activation,\n",
    "                                         training=False)\n",
    "                mc_classifier = eval(model)(classes=n_classes,\n",
    "                                         n_features=len(features),\n",
    "                                         segments_size=int(segments_time * sample_rate),\n",
    "                                         segments_overlap=segments_overlap,\n",
    "                                         decision_size=int(decision_time * sample_rate),\n",
    "                                         decision_overlap=decision_overlap,\n",
    "                                         loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                         loss_function=train_config[tci][\"loss_function\"],\n",
    "                                         lr=lr,\n",
    "                                         beta_1=beta_1,\n",
    "                                         last_activation=last_activation,\n",
    "                                         training=True)\n",
    "                # cross-validation\n",
    "                start = datetime.now()\n",
    "                all_statistics, valid_ratio_tests = h_block_analyzer(db_path=dataset,\n",
    "                                                                     sample_rate=sample_rate,\n",
    "                                                                     features=features,\n",
    "                                                                     n_classes=n_classes,\n",
    "                                                                     noise_rate=noise_rate,\n",
    "                                                                     label_noise_rate=label_noise_rate,\n",
    "                                                                     segments_time=segments_time,\n",
    "                                                                     segments_overlap=segments_overlap,\n",
    "                                                                     decision_time=decision_time,\n",
    "                                                                     decision_overlap=decision_overlap,\n",
    "                                                                     classifier=classifier,\n",
    "                                                                     mc_classifier=mc_classifier,\n",
    "                                                                     last_activation=last_activation,\n",
    "                                                                     calibrator_title=calibrator_title,\n",
    "                                                                     mc_sn=problems[problem]['M'],\n",
    "                                                                     epochs=epochs,\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     data_length_time=data_length_time,\n",
    "                                                                     n_h_block=n_h_block,\n",
    "                                                                     n_train_h_block=n_train_h_block,\n",
    "                                                                     n_valid_h_block=n_valid_h_block,\n",
    "                                                                     n_test_h_block=n_test_h_block,\n",
    "                                                                     h_moving_step=h_moving_step,\n",
    "                                                                     monitor_metric=train_config[tci][\"monitor_metric\"],\n",
    "                                                                     monitor_mode=train_config[tci][\"monitor_mode\"],\n",
    "                                                                     restore_best=restore_best,\n",
    "                                                                     repetitions=repetitions)\n",
    "                valid_ratio_tests_dic[problem] = valid_ratio_tests\n",
    "\n",
    "                end = datetime.now()\n",
    "                running_time = end - start\n",
    "                for prediction_method in all_statistics.keys():\n",
    "                    # Summarizing the results of cross-validation\n",
    "                    data = {}\n",
    "                    data['dataset'] = dataset\n",
    "                    data['prediction_method'] = prediction_method\n",
    "                    data['class'] = str(n_classes)\n",
    "                    loss_metric = train_config[tci][\"loss_metric\"]\n",
    "                    data['loss_metric'] = loss_metric if type(loss_metric) == str else loss_metric.__name__\n",
    "                    loss_function = train_config[tci][\"loss_function\"]\n",
    "                    data['loss_function'] = loss_function if type(loss_function) == str else str(loss_function)\n",
    "                    data['lr'] = lr\n",
    "                    data['beta_1'] = beta_1\n",
    "                    data['monitor_metric'] = train_config[tci][\"monitor_metric\"]\n",
    "                    data['monitor_mode'] = train_config[tci][\"monitor_mode\"]\n",
    "                    data['restore_best'] = str(restore_best)\n",
    "                    data['features'] = str(features)\n",
    "                    data['sample_rate'] = str(sample_rate)\n",
    "                    data['noise_rate'] = str(noise_rate)\n",
    "                    data['label_noise_rate'] = str(label_noise_rate)\n",
    "                    data['epochs'] = str(epochs)\n",
    "                    data['batch_size'] = str(batch_size)\n",
    "                    data['data_length_time'] = str(data_length_time)\n",
    "                    data['repetitions'] = str(repetitions)\n",
    "                    data['n_h_block'] = str(n_h_block)\n",
    "                    data['n_train_h_block'] = str(n_train_h_block)\n",
    "                    data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "                    data['n_test_h_block'] = str(n_test_h_block)\n",
    "                    data['h_moving_step'] = str(h_moving_step)\n",
    "                    data['segments_time'] = str(segments_time)\n",
    "                    data['segments_overlap'] = str(segments_overlap)\n",
    "                    data['inner_classifier'] = str(model)\n",
    "                    data['calibration'] = str(calibrator_title)\n",
    "                    data['M'] = problems[problem]['M']\n",
    "                    data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                    data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "                    data['n_params'] = classifier.count_params()\n",
    "                    data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "                    data['segments_overlap'] = segments_overlap\n",
    "                    data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "                    data['decision_overlap'] = decision_overlap\n",
    "                    statistics_summary = {}\n",
    "                    for key in all_statistics[prediction_method].keys():\n",
    "                        for inner_key in all_statistics[prediction_method][key].keys():\n",
    "                            statistics_summary[key + '_' + inner_key + '_mean'] = np.average(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_std'] = np.std(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_max'] = np.max(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_min'] = np.min(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                    data.update(statistics_summary)\n",
    "                    # Save information\n",
    "                    save_result(log_dir=log_dir, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBk5FG4a6v1x"
   },
   "source": [
    "# Convergance Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "All4BeJqVT_F"
   },
   "outputs": [],
   "source": [
    "CLD_ratio_test = valid_ratio_tests_dic['ConfLongDemo_JSI']\n",
    "HOP_ratio_test = valid_ratio_tests_dic['Healthy_Older_People']\n",
    "UIFW_ratio_test = valid_ratio_tests_dic['User_Identification_From_Walking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WrrB3IPhydhg",
    "outputId": "dd74b171-1bb8-46ed-a4a3-d29777c699d5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family ['normal'] not found. Falling back to DejaVu Sans.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n",
      "WARNING:matplotlib.font_manager:findfont: Font family 'normal' not found.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "plt.figure(figsize=(16,4))\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 22}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "\n",
    "frac = 0.00\n",
    "\n",
    "X = np.concatenate(UIFW_ratio_test,axis=1)\n",
    "# mid = X.mean(axis=1)\n",
    "# mid_s = sm.nonparametric.lowess(mid, list(range(len(mid))), frac = frac)\n",
    "# var = X.var(axis=1)\n",
    "upper = X.max(axis=1)\n",
    "upper_s = sm.nonparametric.lowess(upper, list(range(len(upper))), frac = frac)\n",
    "lower = X.min(axis=1)\n",
    "plt.fill_between(list(range(len(lower))), lower, upper_s[:, 1], alpha=1, label=\"UIFW\", facecolor='yellow', hatch='O')\n",
    "\n",
    "X2 = np.concatenate(HOP_ratio_test,axis=1)\n",
    "# mid2 = X2.mean(axis=1)\n",
    "# mid2_s = sm.nonparametric.lowess(mid2, list(range(len(mid))), frac = frac)\n",
    "# var = X2.var(axis=1)\n",
    "upper = X2.max(axis=1)\n",
    "upper_s = sm.nonparametric.lowess(upper, list(range(len(upper))), frac = frac)\n",
    "lower = X2.min(axis=1)\n",
    "plt.fill_between(list(range(len(lower))), lower, upper_s[:, 1], alpha=1, label=\"HOP\", hatch='X', facecolor='tab:cyan')\n",
    "\n",
    "X3 = np.concatenate(CLD_ratio_test,axis=1)\n",
    "# mid3 = X3.mean(axis=1)\n",
    "# mid3_s = sm.nonparametric.lowess(mid3, list(range(len(mid))), frac = frac)\n",
    "# var = X3.var(axis=1)\n",
    "upper = X3.max(axis=1)\n",
    "upper_s = sm.nonparametric.lowess(upper, list(range(len(upper))), frac = frac)\n",
    "lower = X3.min(axis=1)\n",
    "plt.fill_between(list(range(len(lower))), lower, upper_s[:, 1], alpha=1, label=\"CLD\", hatch='.', facecolor='orange')\n",
    "\n",
    "# plt.plot(mid,linewidth=2,c='red', linestyle='dotted')\n",
    "# plt.plot(mid2,linewidth=2,c='red', linestyle='dashed')\n",
    "# plt.plot(mid3,linewidth=2,c='red', linestyle='dashdot')\n",
    "\n",
    "plt.xlabel(\"Sampling Number\")\n",
    "plt.ylabel(\"$\\sigma$\")\n",
    "plt.ylim(min(lower), 4.5)\n",
    "plt.xlim(0,100)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nolQhVbbq8P"
   },
   "source": [
    "#SOTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z0PEMv-sbsWc",
    "outputId": "449060ca-a0e1-42d9-bdee-32ccc3129fc0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'problems' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m problem \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m     19\u001b[0m       log_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./log/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproblem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m       dataset \u001b[38;5;241m=\u001b[39m \u001b[43mproblems\u001b[49m[problem][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m       n_classes \u001b[38;5;241m=\u001b[39m problems[problem][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_classes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m       features \u001b[38;5;241m=\u001b[39m problems[problem][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'problems' is not defined"
     ]
    }
   ],
   "source": [
    "noise_rate = 100\n",
    "label_noise_rate = 0.0\n",
    "\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "models = ['CNN_L']\n",
    "repetitions = 1\n",
    "restore_best = True\n",
    "lr = 0.0001\n",
    "beta_1 = 0.5\n",
    "\n",
    "calibration_models = ['ENIR']\n",
    "\n",
    "# 'Dataset#2','UCI-HAR-Dataset'\n",
    "datasets = ['Dataset#2']\n",
    "for model in models:\n",
    "  for calibrator_title in calibration_models:\n",
    "    for problem in datasets:\n",
    "          log_dir = f\"./log/{problem}/\"\n",
    "          dataset = problems[problem]['dataset']\n",
    "          n_classes = problems[problem]['n_classes']\n",
    "          features = problems[problem]['features']\n",
    "          sample_rate = problems[problem]['sample_rate']\n",
    "          data_length_time = problems[problem]['data_length_time']\n",
    "          n_h_block = problems[problem]['n_h_block']\n",
    "          n_train_h_block = problems[problem]['n_train_h_block']\n",
    "          n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "          n_test_h_block = problems[problem]['n_test_h_block']\n",
    "          h_moving_step = problems[problem]['h_moving_step']\n",
    "          # train config index\n",
    "          for tci in [0]:\n",
    "              if calibrator_title in ['HistogramBinning', 'IsotonicRegression', 'ENIR', 'BBQ']:\n",
    "                  last_activation = \"softmax\"\n",
    "                  train_config[tci][\"loss_function\"].from_logits = False\n",
    "              else:\n",
    "                  last_activation = \"softmax\"\n",
    "                  train_config[tci][\"loss_function\"].from_logits = False\n",
    "              segments_time = problems[problem][model + '/segments_time']\n",
    "              segments_overlap = problems[problem]['segments_overlaps']\n",
    "              decision_time = problems[problem][model + '/decision_times']\n",
    "              decision_overlap = problems[problem]['decision_overlaps']\n",
    "              classifier = eval(model)(classes=n_classes,\n",
    "                                         n_features=len(features),\n",
    "                                         segments_size=int(segments_time * sample_rate),\n",
    "                                         segments_overlap=segments_overlap,\n",
    "                                         decision_size=int(decision_time * sample_rate),\n",
    "                                         decision_overlap=decision_overlap,\n",
    "                                         loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                         loss_function=train_config[tci][\"loss_function\"],\n",
    "                                         lr=lr,\n",
    "                                         beta_1=beta_1,\n",
    "                                         last_activation=last_activation,\n",
    "                                         training=False)\n",
    "              mc_classifier = eval(model)(classes=n_classes,\n",
    "                                         n_features=len(features),\n",
    "                                         segments_size=int(segments_time * sample_rate),\n",
    "                                         segments_overlap=segments_overlap,\n",
    "                                         decision_size=int(decision_time * sample_rate),\n",
    "                                         decision_overlap=decision_overlap,\n",
    "                                         loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                         loss_function=train_config[tci][\"loss_function\"],\n",
    "                                         lr=lr,\n",
    "                                         beta_1=beta_1,\n",
    "                                         last_activation=last_activation,\n",
    "                                         training=True)\n",
    "                # cross-validation\n",
    "              # cross-validation\n",
    "              start = datetime.now()\n",
    "\n",
    "              add_noise = noise_rate < 100\n",
    "              data_blocks = [i for i in range(n_h_block)]\n",
    "              n_vt = (n_valid_h_block + n_test_h_block)\n",
    "              n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "              i = 0\n",
    "              training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "              train_blocks = training_container[:n_train_h_block]\n",
    "              valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "              test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "              db = Dataset2(dataset,\n",
    "                                sample_rate,\n",
    "                                features=features,\n",
    "                                window_time=segments_time,\n",
    "                                window_overlap_percentage=segments_overlap,\n",
    "                                decision_time=decision_time,\n",
    "                                decision_overlap_percentage=decision_overlap,\n",
    "                                add_noise=add_noise,\n",
    "                                noise_rate=noise_rate,\n",
    "                                label_noise_rate=label_noise_rate,\n",
    "                                train_blocks=train_blocks,\n",
    "                                valid_blocks=valid_blocks,\n",
    "                                test_blocks=test_blocks,\n",
    "                                data_length_time=data_length_time)\n",
    "              db.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "\n",
    "              all_statistics, valid_ratio_tests = h_block_analyzer(db_path=dataset,\n",
    "                                                                     sample_rate=sample_rate,\n",
    "                                                                     features=features,\n",
    "                                                                     n_classes=n_classes,\n",
    "                                                                     noise_rate=noise_rate,\n",
    "                                                                     label_noise_rate=label_noise_rate,\n",
    "                                                                     segments_time=segments_time,\n",
    "                                                                     segments_overlap=segments_overlap,\n",
    "                                                                     decision_time=decision_time,\n",
    "                                                                     decision_overlap=decision_overlap,\n",
    "                                                                     classifier=classifier,\n",
    "                                                                     mc_classifier=mc_classifier,\n",
    "                                                                     last_activation=last_activation,\n",
    "                                                                     calibrator_title=calibrator_title,\n",
    "                                                                     mc_sn=problems[problem]['M'],\n",
    "                                                                     epochs=epochs,\n",
    "                                                                     batch_size=batch_size,\n",
    "                                                                     data_length_time=data_length_time,\n",
    "                                                                     n_h_block=n_h_block,\n",
    "                                                                     n_train_h_block=n_train_h_block,\n",
    "                                                                     n_valid_h_block=n_valid_h_block,\n",
    "                                                                     n_test_h_block=n_test_h_block,\n",
    "                                                                     h_moving_step=h_moving_step,\n",
    "                                                                     monitor_metric=train_config[tci][\"monitor_metric\"],\n",
    "                                                                     monitor_mode=train_config[tci][\"monitor_mode\"],\n",
    "                                                                     restore_best=restore_best,\n",
    "                                                                     repetitions=repetitions)\n",
    "              end = datetime.now()\n",
    "              running_time = end - start\n",
    "              for prediction_method in all_statistics.keys():\n",
    "                    # Summarizing the results of cross-validation\n",
    "                    data = {}\n",
    "                    data['dataset'] = dataset\n",
    "                    data['prediction_method'] = prediction_method\n",
    "                    data['class'] = str(n_classes)\n",
    "                    loss_metric = train_config[tci][\"loss_metric\"]\n",
    "                    data['loss_metric'] = loss_metric if type(loss_metric) == str else loss_metric.__name__\n",
    "                    loss_function = train_config[tci][\"loss_function\"]\n",
    "                    data['loss_function'] = loss_function if type(loss_function) == str else str(loss_function)\n",
    "                    data['lr'] = lr\n",
    "                    data['beta_1'] = beta_1\n",
    "                    data['monitor_metric'] = train_config[tci][\"monitor_metric\"]\n",
    "                    data['monitor_mode'] = train_config[tci][\"monitor_mode\"]\n",
    "                    data['restore_best'] = str(restore_best)\n",
    "                    data['features'] = str(features)\n",
    "                    data['sample_rate'] = str(sample_rate)\n",
    "                    data['noise_rate'] = str(noise_rate)\n",
    "                    data['label_noise_rate'] = str(label_noise_rate)\n",
    "                    data['epochs'] = str(epochs)\n",
    "                    data['batch_size'] = str(batch_size)\n",
    "                    data['data_length_time'] = str(data_length_time)\n",
    "                    data['repetitions'] = str(repetitions)\n",
    "                    data['n_h_block'] = str(n_h_block)\n",
    "                    data['n_train_h_block'] = str(n_train_h_block)\n",
    "                    data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "                    data['n_test_h_block'] = str(n_test_h_block)\n",
    "                    data['h_moving_step'] = str(h_moving_step)\n",
    "                    data['segments_time'] = str(segments_time)\n",
    "                    data['segments_overlap'] = str(segments_overlap)\n",
    "                    data['inner_classifier'] = str(model)\n",
    "                    data['calibration'] = str(calibrator_title)\n",
    "                    data['M'] = problems[problem]['M']\n",
    "                    data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                    data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "                    data['n_params'] = classifier.count_params()\n",
    "                    data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "                    data['segments_overlap'] = segments_overlap\n",
    "                    data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "                    data['decision_overlap'] = decision_overlap\n",
    "                    statistics_summary = {}\n",
    "                    for key in all_statistics[prediction_method].keys():\n",
    "                        for inner_key in all_statistics[prediction_method][key].keys():\n",
    "                            statistics_summary[key + '_' + inner_key + '_mean'] = np.average(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_std'] = np.std(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_max'] = np.max(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                            statistics_summary[key + '_' + inner_key + '_min'] = np.min(\n",
    "                                all_statistics[prediction_method][key][inner_key])\n",
    "                    data.update(statistics_summary)\n",
    "                    # Save information\n",
    "                    save_result(log_dir=log_dir, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qnEf299z3qfE",
    "J3yxSrGp3u-E",
    "rsqjDnvY30UY",
    "XJE0iXH6a0nk",
    "1NRgkdXOYoky",
    "fTAGp_3s364X",
    "9KP0pEQM4KNT",
    "sEfQ8vJyOYDS",
    "_jM3l01B4R-Y",
    "LJsGcm6jBgAL",
    "urkt3wz-Pe_q",
    "JIPRTjzc4eLw",
    "PmNXTb_cuTSB",
    "vDN5yNvo4jZ0",
    "Z9CZOYRH4y8U",
    "0TtSkS-W454c",
    "oo0q8aqH4-4w",
    "LFsacgLZ5EG4",
    "BHeTm0ql5Oye",
    "9EcygQRU5SK_",
    "lBk5FG4a6v1x",
    "3ZGKD5UvLaze",
    "kVsM9Y69O30m"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
